Course: introduction-to-generative-ai-art-of-the-possible
=====
Generative AI is a form of AI that can produce new content, including conversations, stories, images, videos, music, and code.
ML is training a computer to recognize patterns in historical data to make predictions on new data.

AI > ML > DL > Generative AI (here > is super set of right)
Generative AI is powered by large language models that are pretrained on internet-scale data, and these models are called foundation models (FMs).
Generative AI is a subset of deep learning because it can adapt models built using deep learning, but without retraining or fine tuning. 

Eg: Amazon Q Developer is a generative AI tool that increases developer productivity by generating code.

Generative AI services on AWS:
• Foundation Model as a Service: Amazon Bedrock
• Build your own models: 	       Amazon SageMaker, Amazon SageMaker JumpStart(choose one of the language models available and retrain it with your own data.)
• Compute: 			                 AWS Trainium, AWS Inferentia

Risks:
Regulatory requirements
Social risks
Privacy concerns

Benefits:
Personalize customer interactions.
Generate novel content.
Efficiently adapt pre-built models to business use cases.
Achieve productivity gains through automation.
=====


Course: fundamentals-of-machine-learning-and-artificial-intelligence
=====
What is Artificial Intelligence (AI)? – https://aws.amazon.com/what-is/artificial-intelligence/
What is Machine Learning? – https://aws.amazon.com/what-is/machine-learning/
What is Deep Learning? – https://aws.amazon.com/what-is/deep-learning/
What is Generative AI? – https://aws.amazon.com/what-is/generative-ai/

Generation of following types with Generative AI:
Text
Image
Audio
Coding

Issues like:
bias
privacy
responsible use

AI is a broad field that encompasses the development of intelligent systems capable of performing tasks that typically require human intelligence, such as perception, reasoning, learning, problem-solving, and decision-making. AI serves as an umbrella term for various techniques and approaches, including machine learning, deep learning, and generative AI, among others.

ML is a type of AI for understanding and building methods that make it possible for machines to learn. These methods use data to improve computer performance on a set of tasks.

Deep learning uses the concept of neurons and synapses similar to how our brain is wired. An example of a deep learning application is Amazon Rekognition, which can analyze millions of images and streaming and stored videos within seconds.

Generative AI is a subset of deep learning because it can adapt models built using deep learning, but without retraining or fine tuning.
Generative AI systems are capable of generating new data based on the patterns and structures learned from training data.

ML Fundamentals:
===
Building a machine learning model involves:
• data collection and preparation, 
• selecting an appropriate algorithm, 
• training the model on the prepared data, and 
• evaluating its performance through testing and iteration.

Training data:
Bad data is often called garbage in, garbage out, and therefore an ML model is only as good as the data used to train it.

• Labeled data - each input instance has a label that represent output for classification
• Unlabled data

• Structured data    - data with particular format, mostly like form of tables or databases with rows and columns
    Tabular data	   - data stored in spreadsheets, databases, or CSV files
    Time-series data - data consists of sequences of values measured at successive points in time, such as stock prices, sensor readings, or weather data.
• Unstructured data  - data that lacks a predefined structure or format, such as text, images, audio, and video
    Text data        - documents, articles, social media posts, and other textual data.
    Image data       - digital images, photographs, and video frames.

Machine learning process:
  • In supervised learning, the algorithms are trained on labeled data. The goal is to learn a mapping function that can predict the output for new, unseen input data.
  • Unsupervised learning refers to algorithms that learn from unlabeled data. The goal is to discover inherent patterns, structures, or relationships within the input data.
  • In reinforcement learning, the machine is given only a performance score as guidance and semi-supervised learning, where only a portion of training data is labeled. Feedback is provided in the form of rewards or penalties for its actions, and the machine learns from this feedback to improve its decision-making over time.

Inferencing: (process of using the information that a model has learned to make predictions or decisions)
  Syncronous inferencing
    Client-req -> AI -> immediate response
  Asyncronous inferencing - longer processing time
    Client-req -> AI(ack) -> later
  • Batch inferencing     - analyzes large amount of data all at once, where the speed of the decision-making process is not as crucial as the accuracy of the results.
    Eg: data analysis
  • Real-time inferencing - process the incoming data and make a decision almost instantaneously, without taking the time to analyze a large dataset
    Eg: chatbots or self-driving cars


DL Fundamentals:
===
involves the use of Artificial Neural Networks(ANN), which are computational models that are designed to mimic the way the human brain processes information.

Neural networks - core of Deep learning. Like neurons connected in brain, "nodes" are connected here.
  Nodes are organized into layers. The layers include an input layer, one or more hidden layers, and an output layer.

When we show a neural network many examples, like data about customers who bought certain products or used certain services, it figures out how to identify patterns by adjusting the connections between its nodes. It's like the nodes are talking to each other and slowly figuring out the patterns that separate different types of customers.

Couple branches of AI where deep learning is used to enhance results:
  • Computer vision: makes it possible for computers to interpret and understand digital images and videos.
    Deep learning has revolutionized computer vision by providing powerful techniques for tasks such as image classification, object detection, and image segmentation.
  • Natural language processing (NLP): interaction between computers and human languages
    Deep learning has made significant strides in NLP, making possible tasks such as text classification, sentiment analysis, machine translation, and language generation.


Generative AI Fundamentals:
===
emergence of generative AI - due to huge investments in resources
  Hiring a large team, spending on compute resources, and importantly, having the willingness to invest and develop big ideas, are all contributors to the rise of generative AI.

Foundation Models: Generative AI is powered by models that are pretrained on internet-scale data, and these models are called "foundation models (FMs)"

With FMs, instead of gathering labeled data for each model and training multiple models as in traditional ML, you can adapt a single FM to perform multiple tasks.
These tasks include text generation, text summarization, information extraction, image generation, chatbot interactions, and question answering.
FMs can also serve as the starting point for developing more specialized models.

Foundation Model lifecycle:
Data selection:
  Unlabeled data can be used at scale for pre-training, easy to collect than labeled data.
  FMs require training on massive datasets from diverse sources.
Pre-training:
  Although traditional ML models rely on supervised, unsupervised, or reinforcement learning patterns, FMs are typically pre-trained through self-supervised learning.
  With self-supervised learning, labeled examples are not required. Self-supervised learning makes use of the structure within the data to autogenerate labels.

  During the initial pre-training stage, the FM's algorithm can learn the meaning, context, and relationship of the words in the datasets. Like drink means bevarage(noun) or swalloing liquid(verb).

  After the initial pre-training, the model can be further pre-trained on additional data. This is known as "continuous pre-training".
Optimization:
  optimized through techniques like prompt engineering, retrieval-augmented generation (RAG), and fine-tuning on task-specific data.
Evaluation:
  FM's performance can be measured using appropriate metrics and benchmarks.
  Evaluation of model performance and its ability to meet business needs is important.
Deployment:
  deployed in the target production environment.
  Deployment can involve integrating the model into applications, APIs, or other software systems.
Feedback and continuous improvement:
  performance is continuously monitored, and feedback is collected from users, domain experts, or other stakeholders.
  The feedback loop permits continuous enhancement of the foundation model through fine-tuning, continuous pre-training, or re-training, as needed.


Amazon Bedrock provides access to a choice of high-performing FMs from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon.

With these FMs as a foundation, you can further optimize their outputs with prompt engineering, fine-tuning, or RAG.

FMs that are essential to understanding generative AI's capabilities:
==
Large language models (LLM):
  has variety of architecture, most common one is transformer architecture.
  Transformer-based LLMs are powerful models that can understand and generate human-like text.
  They are trained on vast amounts of text data from the internet, books, and other sources, and learn patterns and relationships between words and phrases.

  Tokens are the basic units of text that the model processes.
  Tokens can be words, phrases, or individual characters like a period.
  Tokens also provide standardization of input data, which makes it easier for the model to process.

  Embeddings are numerical representations of tokens, where each token is assigned a vector (a list of numbers) that captures its meaning and relationships with other tokens. These vectors are learned during the training process and allow the model to understand the context and nuances of language.
  For example, the embedding vector for the token "cat" might be close to the vectors for "feline" and "kitten" in the embedding space, indicating that they are semantically related. 

  LLMs use these tokens, embeddings, and vectors to understand and generate text. The models can capture complex relationships in language, so they can generate coherent and contextually appropriate text, answer questions, summarize information, and even engage in creative writing.

Diffusion models:
  starts with pure noise or random data
  models gradually add more and more meaningful information to this noise until they end up with a clear and coherent output, like an image or a piece of text
  
  Forward diffusion: system gradually introduces a small amount of noise to an input image until only the noise is left over.
  Reverse diffusion: noisy image is gradually introduced to denoising until a new image is generated.

  well-kown for text-to-image models

Multimodal models:
  Instead of just relying on a single type of input or output, like text or images, multimodal models can process and generate multiple modes of data simultaneously.
  For example, a multimodal model could take in an image and some text as input, and then generate a new image and a caption describing it as output.

  These kinds of models learn how different modalities like images and text are connected and can influence each other.
  Multimodal models can be used for automating video captioning, creating graphics from text instructions, answering questions more intelligently by combining text and visual info, and even translating content while keeping relevant visuals.
==

Some generative models:
Generative adversarial networks (GANs):
  involves two neural networks competing against each other in a zero-sum game framework. The two networks are generator and discriminator.

  • Generator: This network generates new synthetic data (for example, images, text, or audio) by taking random noise as input and transforming it into data that resembles the training data distribution.
  • Discriminator: This network takes real data from the training set and synthetic data generated by the generator as input. Its goal is to distinguish between the real and generated data.

  During training, the generator tries to generate data that can fool the discriminator into thinking it's real, while the discriminator tries to correctly classify the real and generated data. This adversarial process continues until the generator produces data that is indistinguishable from the real data.
Variational autoencoders (VAEs):
  combines ideas from autoencoders (a type of neural network) and variational inference (a technique from Bayesian statistics).

  • Encoder: This neural network takes the input data (for example, an image) and maps it to a lower-dimensional latent space, which captures the essential features of the data.
  • Decoder: This neural network takes the latent representation from the encoder and generates a reconstruction of the original input data.

  The key aspect of VAEs is that the latent space is encouraged to follow a specific probability distribution (usually a Gaussian distribution), which allows for generating new data by sampling from this latent space and passing the samples through the decoder.


Optimizing model outputs:
Prompt engineering:
  fastest and lowest cost option
  Prompts act as instructions for foundation models. Prompt engineering focuses on developing, designing, and optimizing prompts to enhance the output of FMs for your needs. It gives you a way to guide the model's behavior to the outcomes that you want to achieve.

  • Instructions: This is a task for the FM to do. It provides a task description or instruction for how the model should perform.
  • Context: This is external information to guide the model.
  • Input data: This is the input for which you want a response.
  • Output indicator: This is the output type or format.

  Zero shot - No examples; output based on it's knowledge
    Eg: Translate "hello" to French
  One shot - One example
    Eg: English: Cat, French: Chat; English: Dog, French?
  Two shot - Two examples
    Eg: A dog is a mammal, A cat is a mammal. What is Lion?
  Chain of Thought - Complex -> Simple

  Cyber-attacks in prompt engineering
  1. Prompt Injection
    Manipulate - LLM[behavior]
    Prompts -> manipulate LLM
  2. Prompt Leaking
    LLM(own system prompts)
  3. Prompt Poisioning
    Malicious data -> training data -> manipulated output


Fine-tuning:
  Fine-tuning is a supervised learning process that involves taking a pre-trained model and adding specific, smaller datasets. Adding these narrower datasets modifies the weights of the data to better align with the task.

  • Instruction fine-tuning: uses examples of how the model should respond to a specific instruction. Prompt tuning is a type of instruction fine-tuning.
  • Reinforcement learning from human feedback (RLHF): provides human feedback data, resulting in a model that is better aligned with human preferences.

  Consider this use case for fine-tuning. If you are working on a task that requires industry knowledge, you can take a pre-trained model and fine-tune the model with industry data.
  If the task involves medical research, for example, the pre-trained model can be fine-tuned with articles from medical journals to achieve more contextualized results.

Retrieval-augmented generation (RAG):
  supplies domain-relevant data as context to produce responses based on that data.
  This technique is similar to fine-tuning. However, rather than having to fine-tune an FM with a small set of labeled examples, RAG retrieves a small set of relevant documents and uses that to provide context to answer the user prompt.
  RAG will not change the weights of the foundation model, whereas fine-tuning will change model weights.


<updated till here in my site>

AWS offering ML Frameworks, AI/ML Services and Generative AI:
===
ML Frameworks:
  Amazon Sagemaker - build, train, and deploy ML models for any use case with fully managed infrastructure, tools, and workflows
    provides a complete machine learning lifecycle, including data preparation, model building, training, tuning, and deployment.
AI/ML Services:
  Text and documents:
    Amazon Comprehend - uses ML & Natural Language Processing to uncover insights & relationships of unstructured data, detecting offensive words in text.
      insights and "patterns from textual data", such as sentiment analysis, language detection, key phrase extraction, and more. One of its important features includes "Toxic Content Detection", which can be used to moderate content and detect offensive language.
    Amazon Translate - Neural machine translation using deep learning to translate
    Amazon Textract - extract printed text, handwriting, layout elements, and data from any document
  Chatbots:
    Amazon Lex - design, build, test, and deploy conversational interfaces into any application using voice and text
      deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text.
      the same deep learning technologies that power Amazon Alexa are now available to any developer. You can efficiently build sophisticated, natural-language conversational bots and voice-enabled interactive voice response (IVR) systems.
  Speech:
    Amazon Polly - Text to speech, natural-sounding human voices in dozens of languages.
    Amazon Transcribe - automatic speech recognition (ASR) service for automatically converting speech to text
  Vision:
    Amazon Rekognition - image recognition and video analysis with machine learning.
      can identify objects, people, text, scenes, and activities in images and videos, and even detect inappropriate content.
      facial analysis and facial search capabilities
      detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.
  Search:
    Amazon Kendra - use ML. Intelligent search across organizational data and other webs, etc
  Recommendations:
    Amazon Personalize
      provide an activity stream from your application (page views, signups, purchases, and so forth)
      provide an inventory of the items that you want to recommend, such as articles, products, videos, or music
      choose to provide additional demographic information from your users, such as age or geographic location
      Amazon Personalize processes and examines the data, identifies what is meaningful, selects the right algorithms, and trains and optimizes a personalization model that is customized for your data.
  Misc:
    AWS DeepRacer - to get started with reinforcement learning (RL)
      RL superpower is that it learns very complex behaviors without requiring any labeled training data, and it can make short-term decisions while optimizing for a longer-term goal.
Generative AI:
  Amazon Sagemaker JumpStart - choose one of the language models available and retrain it with your own data
    for accelerating model development and deployment
    supports one-click deployment and fine-tuning of more than 150 popular open-source models(most common use cases) such as natural language processing, object detection, and image classification models.
  Amazon Bedrock - Foundation Model as a Service
    offers a choice of high-performing FMs from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and AWS, through a single API
  Amazon Q
    help you get fast, relevant answers to pressing questions, solve problems, generate content, and take actions using the data and expertise found in your company's information repositories, code, and enterprise systems
    When you chat with Amazon Q, it provides immediate, relevant information and advice to help streamline tasks, speed decision-making, and help spark creativity and innovation.
  Amazon Q Developer - improve developer productivity
    provides ML–powered code recommendations to accelerate development of C#, Java, JavaScript, Python, and TypeScript applications.
    integrates with multiple integrated development environments (IDEs) and helps developers write code faster by generating entire functions and logical blocks of code—often consisting of more than 10–15 lines of code.

Advantages and benefits of AWS AI solutions:
Accelerated development and deployment
  Amazon Q Developer can generate code in real time
  SageMaker handles tasks such as data preprocessing, model training, and deployment.
  Amazon Bedrock provides access to pre-trained models and APIs
Scalability and cost optimization
  pay-as-you-go pricing models
  scale seamlessly across regions and handle large datasets or high-volume traffic.
Flexibility and access to models
  AWS continuously updates and expands its AI services, providing access to the latest advancements in machine learning models, techniques, and algorithms.
  Amazon Bedrock offers a choice of high-performing FMs from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and AWS, through a single API.
Integration with AWS tools and services
  Amazon Comprehend and Amazon Rekognition offer ready-to-use AI capabilities that can be readily incorporated into applications.
  AWS AI services seamlessly integrate with other AWS services, so developers can build end-to-end solutions that use multiple cloud services.
  AWS ecosystem provides a wide range of tools, SDKs, and APIs, so developers can incorporate AI capabilities into their existing applications or build entirely new AI-driven applications.

Cost considerations:
  Responsiveness and availability
    higher levels of responsiveness and availability often come at an increased cost
  Redundancy and Regional coverage
    comes with an additional cost, because resources have to be provisioned and data replicated across multiple locations.
  Performance
    Higher-performance options, such as GPU instances, generally come at a higher cost
  Token-based pricing
    Many AWS generative AI services, such as Amazon Q Developer and Amazon Bedrock, use a token-based pricing model. This means that you pay for the number of tokens (a unit of text or code) generated or processed by the service. The more tokens you generate or process, the higher the cost.
  Provisioned throughput
    Some AWS generative AI services, like Amazon Polly and Amazon Transcribe, let you provision a specific amount of throughput (for example, audio or text processing capacity) in advance. Higher provisioned throughput levels typically come at a higher cost
  Custom models
    AWS provides pre-trained models for various generative AI tasks, but you can also bring your own custom models or fine-tune existing models with additional costs
=====


Course: exploring-artificial-intelligence-use-cases-and-applications
=====
AI, ML, and generative AI real-world use cases, capabilities and limitations, model selection techniques, and key business metrics.

AI transformed the industries by boosting employee productivity, fostering creativity, improving business operations, and enhancing customer experiences.

Text summarization, code generation, image, video, and text generation, music creation, content localization, chatbots, virtual assistance, anomaly detection, maintenance assistance, and contact center analytics are ways AI transforms how businesses operate and make decisions.

Examples of Real-World Use Cases:
<in detail in course>
Media and entertainment
  Content generation
  Virtual reality
  New generation
Retail
  Product review summaries
  Pricing optimization
  Virtual try-ons
  Store layout optimization
Healthcare
  AWS HealthScribe
  Personalize medicine
  Improve medical imaging
Life sciences
  Drug discovery
  Protein folding prediction
  Synthetic biology
Financial services
  Fraud detection mechanisms
  Portfolio management
  Debt collection
Manufacturing
  Predictive maintenance
  Process optimization
  Product design
  Material science


Examples of AI Applications:
Computer vision
  allows computers to interpret and understand digital images and videos.
Natural language processing
  interaction between computers and human languages
  perform tasks such as text classification, sentiment analysis, machine translation, and language generation.
Intelligent document processing (IDP)
  extracts and classifies information from unstructured data, generates summaries, and provides actionable insights.
Fraud detection
  process of identifying and preventing fraudulent activities or unauthorized behavior with a system, process, or transaction.


ML is a subset of AI that focuses on developing algorithms and statistical models so that computer systems can learn from data and make predictions or decisions without being explicitly programmed.

When AI and ML are appropriate solutions
  Coding the rules is challenging
  Scale of the project is challenging

ML Techniques and Use Cases:
  Supervised - takes labeled data
    Classification - assign labels or categories to new, unseen data instances based on a trained model
      Fraud detection
      Image classification
      Customer retention
      Diagnostics
    Regression     - predicting continuous or numerical values
      Advertising popularity prediction
      Weather forecasting
      Market forecasting
      Estimating life expectancy
      Population growth prediction
  Unsupervised - takes unlabeled data
    Clustering              - groups data into different clusters based on similar features or distances between the data point to better understand the attributes of a specific cluster.
      Customer segmentation
      Targeted marketing
      Recommended systems
    Dimensionally Reduction - reduce the number of features or dimensions in a dataset while preserving the most important information or patterns.
      Big data visualization
      Meaningful compression
      Structure discovery
      Feature elicitation
  Reinforcement - continuously improves its model by mining feedback from previous iterations
      AWS DeepRacer simulator


Generative AI is a subset of deep learning. It can adapt models that are built using deep learning without needing to retrain or fine-tune them

Capabilities of Generative AI:
  Adaptability
    adapt to various tasks and domains by learning from data and generating content tailored to specific contexts or requirements
  Responsiveness
    can generate content in real-time, which results in rapid response times and dynamic interactions
    useful for chatbots, virtual assistants, and other interactive applications that require immediate responses.
  Simplicity
    can simplify complex tasks by automating content creation processes
    Eg: can generate human-like text, which reduces the time and effort required for content generation.
  Creativity and exploration
    can generate novel ideas, designs, or solutions by combining and recombining elements in unique ways.
  Data efficiency
    can learn from relatively small amounts of data and generate new samples consistent with the training data
  Personalization
    can create personalized content tailored to individual preferences or characteristics, which enhances user experiences and engagement.
  Scalability
    can generate large amounts of content quickly. This makes the models suitable for tasks that require producing content at scale.

Challenges of Generative AI:
  Regulatory violations
    when trained with sensitive data might generate output that violates regulations, such as exposing PII
  Social risks
    possibility of unwanted content that might reflect negatively on your organization
  Data security and privacy concerns
    The information shared with your model can include personal information and can potentially violate privacy laws.
  Toxicity
    can generate content that is inflammatory, offensive, or inappropriate.
  Hallucinations
    model generates inaccurate responses that are not consistent with the training data, called hallucinations
  Interpretability
    Users might misinterpret the model’s output, which could lead to incorrect conclusions or decisions.
  Nondeterminism
    model might generate different outputs for the same input, which can cause problems in applications where reliability is key.

<Each challenge's solution in course>

Factors to Consider When Selecting a Generative AI Model:
  Models are optimized for different tasks, so choosing the right one is crucial for achieving the desired results.

  Model types
    AI21 labs - Jurassic-2 Models
    Amazon - Amazon Titan
    Anthropic - Claude
    Stability AI - Stable Diffusion
    Cohere - Command
    Meta - Llama
  Performance requirements
    requirements include accuracy, reliability of the output, and others.
    Assess the overall performance of the model to evaluate its suitability for a particular task.
    You should also test the model against different datasets to ensure reliability.
    Finally, monitor its performance over time to ensure it remains consistent. 
  Capabilities
    Generative AI can perform different tasks with varying degrees of output quality and levels of control or customization.
  Constraints
    Computational resources (for example, available GPU power, CPU power, or memory)
    Data availability (for example, size and quality of training data)
    Deployment requirement (for example, on premises or cloud)

    Some models might have higher resource demands or require specific hardware configurations, which could impact their use case.
  Compliance
    Generative AI models can pose moral concerns, including biases, privacy issues, and potential misuse. When evaluating a particular model, consider its compliance and moral implications, particularly in sensitive domains like healthcare, finance, and legal applications. One should consider factors such as fairness, transparency or traceability, accountability, hallucination, and toxicity. Additionally, the model should adhere to relevant regulation guidelines.
  Cost
    Larger models are usually more precise, but they are expensive and offer few deployment options. Conversely, smaller models are cheaper and faster, and they offer more deployment alternatives.
    By using generative AI for content creation, you can reduce labor costs and increase efficiency, especially for repetitive tasks that require significant human effort.
    Remember to evaluate all expenses related to deployment, maintenance, hardware, software, and other associated costs.

Business Metrics for Generative AI
  By quantifying the performance, effectiveness, and return on investment (ROI) of AI applications through relevant business metrics, organizations can gain valuable insights into the value delivered. They can also identify areas of improvement and make informed decisions to optimize resource allocation and strategy.

  User satisfaction
    gathers user feedback to assess their satisfaction with the AI-generated content or recommendations.
  Average revenue per user (ARPU)
    calculates the average revenue generated per user or customer attributed to the generative AI application.
  Cross-domain performance
    measures the generative AI model's ability to perform effectively across different domains or industries.
  Conversion rate
    monitors the conversion rate to generate content or recommend desired outcomes, such as purchases, sign-ups, or engagement metrics.
  Efficiency
    metric evaluates the generative AI model's efficiency in resource utilization, computation time, and scalability.
  Customer lifetime value(CLV)
    measures the total revenue a company can expect to generate from a single customer throughout their entire relationship with the business
=====


Course: developing-machine-learning-solutions
=====
* When doing recap see Conclusion section in course first.

https://aws.amazon.com/sagemaker-ai/getting-started/

Will learn about the machine learning lifecycle, and how to use AWS services at every stage. Additionally, you will discover the diverse sources for machine learning models and learn techniques to evaluate their performance. You will also understand the importance of machine learning operations (MLOps) in streamlining the development and deployment of your machine learning projects.

Machine Learning Development Lifecycle:
  Business goal identification
  ML problem framing
  Data processing (
    data collection and integration - raw data in central place,
    data preprocessing and data visualization - raw data into an understandable format, and 
    feature engineering - creating, transforming, extracting, and selecting variables from data)
  Model development (training, tuning, and evaluation)
    do additional feature engineering and tune the model's hyperparameters before retraining
  Model retraining
    might also involve adjusting the training hyperparameters
  Model deployment (inference and prediction)
  Model monitoring
    ensures the model is maintaining a desired level of performance through early detection and mitigation
  Iterations
    Add new data and retrain
    helps ensure that the model remains accurate and relevant over time.

Developing ML Solutions with Amazon SageMaker:
Amazon SageMaker is a fully managed ML service.

• Collect and prepare data.
• Build and train machine learning models.
• Deploy the models and monitor the performance of their predictions.

Collecting, analyzing, and preparing your data
  "Amazon SageMaker Data Wrangler" - import, prepare, "transform", featurize, and analyze data by using a web interface
    low-code no-code (LCNC) tool
    For more advanced users and data preparation at scale, can use built-in integration of Amazon EMR and AWS Glue interactive sessions
    using the "SageMaker Processing API", customers can run scripts and notebooks to process, transform, and analyze datasets
    can also use various ML frameworks such as scikit-learn, MXNet, or PyTorch while benefiting from fully managed machine learning environments.
Managing Features
  "Amazon SageMaker Feature Store" - helps create, share, and manage features for ML development
Model training and evaluation
  SageMaker provides a training job feature to train and deploy models using built-in algorithms or custom algorithms.
  SageMaker launches the ML compute instances and uses the training code and the training dataset to train the model. It saves the resulting model artifacts in an Amazon S3 bucket that can be used later for inference.
  "Amazon SageMaker Canvas" - use machine learning to generate predictions without needing to write any code. For customers aiming at a LCNC option.
  "Amazon SageMaker JumpStart" - provides pretrained, open source models that customers can use for a wide range of problem types.
Model evaluation
  "Amazon SageMaker Experiments"
    experiment with multiple combinations of data, algorithms, and parameters, all while observing the impact of incremental changes on model accuracy.
  "Amazon SageMaker Automatic Model Tuning"
    Hyperparameter tuning(way to find the best version of your models) by running many jobs with different hyperparameters in combination and measuring each of them by a metric that you choose.
Deployment
  With SageMaker, customers can deploy their ML models to make predictions, also known as "inference". SageMaker provides a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs.
Monitoring
  "Amazon SageMaker Model Monitor" - used to observe quality of SageMaker ML models in production.
    They can set up continuous monitoring or on-schedule monitoring.
    Helps maintain model quality by detecting violations of user-defined thresholds for data quality, model quality, bias drift, and feature attribution drift.

"Amazon SageMaker Studio" - a web-based UI that provides access to all SageMaker environments and resources


Sources of ML Models:
Model implementations
SageMaker supports
  pre-trained models, 
  built-in algorithms, and 
  custom Docker images. 

The following are ways to use SageMaker to build your ML model:
  • Pre-trained models require the least effort and are models ready to deploy or to fine-tune and deploy using "SageMaker JumpStart".
  • Built-in models available in SageMaker require more effort and scale if the dataset is large and significant resources are needed to train and deploy the model.
  • If there is no built-in solution that works, try to develop one that uses pre-made images for machine learning and deep learning frameworks for supported frameworks such as scikit-learn, TensorFlow, PyTorch, MXNet, or Chainer.
  • You can build your own custom Docker image that is configured to install the necessary packages or software. 

SageMaker built-in algorithms
  Supervised Learning
    Classification, Regression - Linear learner, Factorization machines, XGBoost, K-Nearest Neighbors(KNN)
  Unsupervised learning
    Clustering - K-means, Latent Dirichlet Allocation (LDA)
    Topic modeling - Latent Dirichlet Allocation (LDA)
    Embeddings - Object2Vec
    Anomaly detection - Random cut forest, IP insights
    Dimensionality reduction - Principal Component Analysis
  Image processing
    Images/Videos
      Image classification, Object detection - MXNet TensorFlow(ResNet, ImageNet, ...)
      Semantic segmentation - Fully Convolutional Network(FCN), Pyramid Scene Parsing(PSP), DeepLab V3 with ResNet
    Time series - DeepAR
  Text analysis
    Text
      Text classification, Word2Vec - BlazingText
      Machine translation - Sequence to sequence
      Topic modeling - Latent Dirichlet Allocation (LDA), Neural topic Modelling (NTM)
    Speech - Sequence to sequence

SageMaker Jumpstart
  You can deploy, fine-tune, and evaluate pre-trained models from the most popular model hubs.
  SageMaker JumpStart provides pretrained open source models from leading providers for a range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. SageMaker JumpStart also provides solution templates that set up infrastructure for common use cases and runnable example notebooks for machine learning with SageMaker.

Summary of Machine learning models
SageMaker supports pre-trained models, built-in algorithms, and custom Docker images. It provides several built-in general-purpose algorithms that can be used for the following:
  • Supervised learning with either classification or regression problems
  • Unsupervised learning for tasks such as clustering, dimension reduction, pattern recognition, and anomaly detection
  • Image processing for image classification, object detection, and computer vision as well as time series
  • The analysis of textual documents used in natural language processing, document classification or summarization, topic modeling or classification, and language transcription or translation

Machine Learning Models Performance Evaluation:
Model evaluation datasets
  Training set - 70% - used to train
  Validation set - 10% - improve the model before in production
  Testing set - 10% - evaluate the predictive quality
Model fit
  Overfitting - perform well on training data, not perform well on evaluation data
  Underfitting - performs poorly on training data
  Balanced - not overfit or underfit to training data
Bias and variance
  Consider the center of the bullseye in this situation is the label or target.
  Think about bias as the gap between your predicted value and the actual value, whereas variance describes how dispersed your predicted values are.
  In ML, the ideal algorithm has low bias and can accurately model the true relationship. The ideal algorithm also has low variability, by producing consistent predictions across different datasets.

  Overfitting - High variance
  Underfitting - High bias
  Balanced - Low bias, Low variance

Classification problem metrics
  Classification example - compare actual value vs prediction value of cat or not cat.

  • Accuracy using Confusion matrix
    True Positive(TP) - actual positive, prediction positive - good
    False Positive(FP) - actual negative, prediction positive
    False Negative(FN) - actual postive, prediction negative
    True Negative(TN) - actual negative, prediction negative - good
  Calculation for a model's accuracy = (TP + TN) / (TP + TN + FP + FN)
  This metric is less effective when there are a lot of true negative cases in your dataset.

  • Precision - removes the negative predictions from the picture.
  Calculation for precision = TP / (TP + FP)
  When the cost of false positives are high in your particular business situation, precision can be a good metric.
  Think about a classification model that identifies emails as spam or not. In this case, you do not want your model labeling a legitimate email as spam and preventing your users from seeing that email.
  FP - No, but Yes said count

  • Recall(or Sensitivity) - how good the algorithm is at detecting, for example, cats.
  Calculation for recall = TP / (TP + FN)
  Think about a model that needs to predict whether a patient has a terminal illness or not. In this case, using precision as your evaluation metric does not account for the false negatives in your model. It is extremely important and vital to the success of the model that it not give false negative results.
  FN - Yes, but No said count

  • Area under the curve-receiver operator curve (AUC-ROC)
  ROC is a probability curve, and AUC represents the degree or measure of separability.
  AUC-ROC uses sensitivity (true positive rate) and specificity (false positive rate)
  In general, AUC-ROC can show what the curve for true positive compared to false positive looks like at various thresholds. That means that when you calculate the AUC-ROC curve, you plot multiple confusion matrices at different thresholds and compare them to one another to find out the threshold you need for your business use case.

Regression problem metrics
  • Mean squared error
  you take the difference between the prediction and actual value, square that difference, and then sum up all the squared differences for all the observations.
  The smaller the MSE, the better the model's predictive accuracy.

  • R squared - commonly used metric with linear regression problems
  R squared explains the fraction of variance accounted for by the model. It’s like a percentage, reporting a number from 0 to 1. When R squared is close to 1, it usually indicates that a lot of the variance in the data can be explained by the model itself.

  MSE focuses on the average squared error of the model's predictions to provide a measure of model performance.
  R squared provides a measure of the model's goodness of fit to the data.

Business metrics
KPIs are established in the business goal identification phase. They can include goals such as increasing sales, cutting costs, or decreasing customer churn.

Evaluate whether the performance metrics accurately reflect the business’ tolerance for the error. For instance, false positives might lead to excessive maintenance costs in predictive maintenance use cases. Another example is deciding if acquiring a new customer is more expensive than retaining one. A business should focus on numerical metrics, such as precision and recall, to help differentiate the business requirements and be closer aligned to business value. 

Consider developing custom metrics that tune the model directly for the business objectives. One way is to develop a cost function to evaluate the economic impact of the model. For the cost function, you can specify the cost, or value, of correct predictions and the cost of errors.

By using A/B testing or the canary deployments technique, developers can experiment with two or more variants of a model and help achieve the business goals.

Model Deployment:
Model deployment is the integration of the model and its resources into a production environment so that it can be used to create predictions.

  • Self-hosted API approach - deploy and host your ML models on your own infrastructure, either on premises or in the cloud
  • Managed API services are cloud-based services that provide a fully managed environment for deploying and hosting your ML models as APIs. SageMaker is an example. These services abstract away the underlying infrastructure management so you can focus on building and deploying your models.

  Advantages of self-hosted APIs include greater control over the infrastructure, potential cost savings (depending on usage), and the ability to customize the deployment environment. However, this approach requires more operational overhead and responsibility for managing and maintaining the infrastructure.

Sagemaker provides the following:
  • Deployment with one click or a single API call
  • Automatic scaling
  • Model hosting services
  • HTTPS endpoints that can host multiple models

Sagemaker deployment options:
  • Real-time
    Real-time inference is ideal for inference workloads where you have real-time, interactive, and low latency requirements.
  • Batch transform
    Use batch transform when you need to get inferences from large datasets and don't need a persistent endpoint. You can also use it when you need to preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.
  • Asynchronous
    SageMaker asynchronous inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements.
  • Serverless
    On-demand serverless inference is ideal for workloads that have idle periods between traffic spurts and can tolerate cold starts. It is a purpose-built inference option that you can use to deploy and scale ML models without configuring or managing any of the underlying infrastructure.

Fundamental Concepts of MLOps:
MLOps combines people, technology, and processes to deliver collaborative ML solutions.
MLOps goes from model development to model monitoring.
MLOps refers to the practice of operationalizing and streamlining the end-to-end machine learning lifecycle from model development and deployment to monitoring and maintenance.

MLOps accounts for the unique aspects of AI/ML projects in project management, CI/CD, and quality assurance.

Goals of MLOps
A goal of MLOps is to get ML workloads into production and keep them operating.
The aim is to use MLOps to do the following:
  • Increase the pace of the model development lifecycle through automation.
  • Improve quality metrics through testing and monitoring.
  • Promote a culture of collaboration between data scientists, data engineers, software engineers, and IT operations.
  • Provide transparency, explainability, audibility, and security of the models by using model governance.

Benefits of MLOps
  Productivity
    By providing self-service environments with access to curated datasets, data engineers and data scientists can move faster and waste less time with missing or invalid data.
  Reliability
    By incorporating CI/CD practices, developers can deploy quickly with increased quality and consistency.
  Repeatability
    By automating all the steps in the machine learning development lifecycle, you can ensure a repeatable process, including how the model is trained, evaluated, versioned, and deployed.
  Auditability
    By versioning all inputs and outputs, from data science experiments to source data to trained models, you can demonstrate exactly how the model was built and where it was deployed.
  Data and Model quality
    With MLOps, you can enforce policies that guard against model bias and track changes to data statistical properties and model quality over time.

Key principles of MLOps
Version control
  For reproducibility, machine learning workflows must track changes to assets like data, code, and models. It can be rolled back to previous versions when needed.
  Overall, version control and code review provide reproducible, trustworthy machine learning.
Automation
  For repeatability, consistency, and scalability, you can automate the various stages in the machine learning pipeline. This includes the data ingestion, pre-processing, model training, and validation and deployment stages. 

  Automated testing helps you discover problems early for fast error fixes and learnings.
CI/CD
  Through automation, you can continuously test and deploy assets in the following ways:
  • Continuous integration extends the validation and testing of code to data and models in the pipeline.
  • Continuous delivery automatically deploys the newly trained model or model prediction service.
  • Continuous training automatically retrains ML models for redeployment.
  • Continuous monitoring uses data monitoring and model monitoring of metrics related to business.
Model governance
  Good governance of machine learning systems requires close collaboration between data scientists, engineers, and business stakeholders. Clear documentation, effective communication channels, and feedback mechanisms help align everyone and improve models over time. It is also crucial to protect sensitive data, secure access, and meet compliance rules. A structured process for reviewing, validating, and approving models before deployment checks for fairness, bias, and ethics. Governance manages all aspects of systems for efficiency.

ML lifecycle and MLOps
Managing code, data, and models throughout the ML lifecycle requires the following touchpoints:
• Processing code in data preparation
• Training data and training code in model building
• Candidate models, test, and validation data in model evaluation
• Metadata during model selection
• Deployment-ready models and inference code during deployment
• Production code, models, and data for monitoring

Implementing MLOps
A productionized ML lifecycle typically contains separate training and deployment pipelines.
• Model build
    The model building pipeline creates new models upon inititation, for example when new data become available.
• Model evaluation
    When the model building pipeline completes, you can implement quality control measures at the model registration step. The quality control step can be either manual (human in the loop) or automated. If a model meets baseline performance metrics, it can be registered with a model registry.
• Model approval
    You can use the registry to approve or reject model versions. The model approval action can act as an initiation to start the deployment pipeline.
• Model deployment
    The deployment pipeline is most similar to traditional CI/CD systems. This pipeline includes steps such as the following: 
    • Source
    • Build 
    • Deployment to staging environment 
    • Testing
    • Promotion to production environment
• Model monitoring
    As soon as the model is in production, you should get feedback from the live system. For ML solutions, monitor the hosting infrastructure, data quality, and model performance.

AWS services for MLOps
<AWS_MLOPS_Services2_NOPROCESS_.png>
=====


Course: developing-generative-ai-solutions
=====
Capabilities:
• Adaptability
• Responsiveness
• Simplicity
• Creativity and exploration
• Data efficiency
• Personalization
• Scalability

Challenges:
• Regulatory violations
• Social risks
• Data security and privacy concerns
• Toxicity
• Hallucinations
• Interpretability
• Nondeterminism

Generative AI application lifecycle:
• Defining a business use case
  In the first stage, requirements for incorporating generative AI capabilities into an application are identified. This might involve analyzing the application's functionalities, user needs, and business goals to determine where generative AI can add value.
• Selecting a foundation model (FM)
  Based on the identified requirements, an appropriate generative AI model is either selected from existing pre-trained models or developed from scratch. This decision depends on factors such as the availability of suitable pre-trained models, the complexity of the use case, and the availability of domain-specific data for training.
• Improving the performance of an FM
  The selected or developed generative AI model is integrated into the application's codebase or infrastructure. This might involve adapting the model's input and output formats, fine-tuning the model with application-specific data, and implementing any necessary customizations or optimizations.
• Evaluating the performance of an FM
  Thorough testing and evaluation of the integrated generative AI capabilities are conducted to ensure that they meet the specified requirements and perform as expected. This might involve testing with various inputs, edge cases, and real-world scenarios, as well as evaluating the quality, coherence, and relevance of the generated content.
• Deployment and its impact on business objectives
  After successful testing, the application with integrated generative AI capabilities is deployed to the production environment. Monitoring mechanisms are established to track the performance, usage, and potential issues or biases associated with the generative AI model's outputs.

Defining a Use Case:
  Parts of a use case:
    • Use case name
    • Brief description
    • Actors
    • Preconditions
    • Basic flow (main success scenario)
    • Alternative flows (extensions)
    • Postconditions
    • Business rules
    • Nonfunctional requirements
    • Assumptions
    • Notes or additional information

  Key metrics:
    • Cost savings
    • Time savings
    • Quality improvement
    • Customer satisfaction
    • Productivity gains

  Approaches:
    • Process automation
    • Automated decision-making
    • Personalization and customization
    • Creative content generation
    • Exploratory analysis and innovation

Selecting an FM:
One key consideration is whether to use pre-trained models or develop a model from scratch.

3.1.a
Pre-trained model selection criteria
Pre-trained models offer a valuable head start by encapsulating knowledge distilled from vast amounts of data. These models can be fine-tuned on task-specific data, potentially leading to faster convergence and better generalization. However, pre-trained models might carry undesirable biases or fail to fully capture the nuances of the target domain.

Selection criteria for choosing a pre-trained model depend on the requirements of the business use case. Some are:
• Cost
• Modality
• Latency
• Multi-lingual support
• Model size
• Model complexity
• Customization
• Input/output length
• Responsibility considerations
• Deployment and integration

Choosing a pre-trained model based on selection criteria
Some of pre-trained models available on Amazon Bedrock:
  AI21 labs - Jurassic-2 Models
  Amazon - Amazon Titan
  Anthropic - Claude
  Cohere - Command
  Meta - Llama
  Mistral AI - Mistral Large
  Stability AI - Stable Diffusion

  <Explained details in course>

Regularly reviewing and updating the selection criteria as new models and techniques emerge is recommended, because the generative AI landscape is rapidly evolving.

Improving the Performance of an FM
Prompt engineering
  Prompt engineering is the fastest way to harness the power of large language models (LLMs). By interacting with an LLM through prompts (a series of questions, statements, or instructions), you can adjust LLM output behavior based on the specific context of the output that you want to achieve.

  Some key aspects of prompt engineering:
    • Design: Crafting clear, unambiguous, and context-rich prompts that effectively communicate the desired task or output to the model
    • Augmentation: Incorporating additional information or constraints into the prompts, such as examples, demonstrations, or task-specific instructions, to guide the model's generation process
    • Tuning: Iteratively refining and adjusting the prompts based on the model's outputs and performance, often through human evaluation or automated metrics
    • Ensembling: Combining multiple prompts or generation strategies to improve the overall quality and robustness of the outputs
    • Mining: Exploring and identifying effective prompts through techniques like prompt searching, prompt generation, or prompt retrieval from large prompt libraries

Prompt techniques
  • Zero-shot prompting
  • Few-shot prompting
  • Chain-of-thought (CoT) prompting
  • Self-consistency
  • Tree of thoughts (ToT)
  • Retrieval Augmented Generation (RAG)
  • Automatic Reasoning and Tool-use (ART)
  • ReAct prompting

  Zero shot - No examples; output based on it's knowledge
    Eg: Translate "hello" to French
  One shot - One example
    Eg: English: Cat, French: Chat; English: Dog, French?
  Two shot - Two examples
    Eg: A dog is a mammal, A cat is a mammal. What is Lion?
  Chain of Thought - Complex -> Simple

3.1.c
Retrieval Augmented Generation (RAG)
RAG is a natural language processing (NLP) technique that combines the capabilities of retrieval systems and generative language models to produce high-quality and informative text outputs.

The RAG prompt techniques approach uses:
  The "retrieval system" provides access to a vast amount of factual knowledge and information.
  And the "generative language model" can synthesize and present this information in a natural and coherent manner, tailored to the specific input or context.

some RAG business applications
• Building intelligent question-answering systems 
• Expanding and enriching existing knowledge bases 
• Generating high-quality content

some examples of Amazon Bedrock knowledge bases that could be applicable to Retrieval Augmented Generation (RAG) business use cases:
• Customer service chatbot
• Legal research and analysis
• Healthcare question-answering

Fine-tuning
  Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific task or domain-specific dataset.
  Although FMs are pre-trained through self-supervised learning and have inherent capability of understanding information, fine-tuning the FM base model can improve performance.

  Two ways to fine-tune a model:
    1. Instruction fine-tuning uses examples of how the model should respond to a specific instruction. Prompt tuning is a type of instruction fine-tuning.
    2. Reinforcement learning from human feedback (RLHF) provides human feedback data, resulting in a model that is better aligned with human preferences.

  Let's consider a use case for fine-tuning. If you are working on a task that requires industry knowledge, you can take a pre-trained model and fine-tune the model with industry data.

  The pre-trained model's architecture is often modified by adding additional layers or components specific to the target task.

  The pre-trained model, with the added task-specific layers, is then fine-tuned on the task-specific dataset. During fine-tuning, the model's parameters are updated to better capture the patterns and nuances present in the task-specific data.

  This approach is particularly useful when the target task has a limited amount of training data.

Creating a foundation model from scratch:
This approach is typically undertaken when there are no suitable pre-trained models available for the specific task or domain, or when the requirements for accuracy, performance, or customization are exceptionally high.

defining the model architecture, which involves selecting the appropriate neural network architecture, layers, and hyperparameters based on the problem at hand
Next, a large and diverse dataset must be curated, cleaned, and preprocessed to serve as the training data for the model.
model is initialized with random weights and trained using various optimization algorithms in an iterative process.
Creating a model from scratch allows for complete customization and tailoring to the specific problem. But it comes at a significant "cost in terms of computational resources, time, and expertise required".

3.1.e

3.1.f
Automated multi-step tasks with agents:
Agents are software components or entities designed to perform specific actions or tasks autonomously or semi-autonomously, based on predefined rules or algorithms.

By using agents for multi-step tasks, organizations can achieve higher levels of automation, consistency, and efficiency in their cloud operations, while also improving visibility, control, and auditability of the processes involved.

some examples of tasks that agents can accomplish:
• Task coordination
• Reporting and logging
• Scalability and concurrency
• Integration and communication


Evaluating an FM:
3.4.a (1)
Types of evaluation methods
Human evaluation
  Human evaluators can provide qualitative feedback on factors like coherence, relevance, factuality, and overall quality of the model's outputs. Although human evaluation is often considered the gold standard, it can be time consuming and expensive, especially for large-scale evaluations.
Benchmark datasets
  Benchmark datasets are curated collections of data designed specifically for evaluating the performance of language models or other AI systems. These datasets often consist of carefully selected examples or tasks that cover a wide range of topics, complexities, and linguistic phenomena.

  Some popular benchmark datasets for natural language processing tasks:
    • The General Language Understanding Evaluation (GLUE) benchmark is a collection of datasets for evaluating language understanding tasks like text classification, question answering, and natural language inference.
    • SuperGLUE is an extension of GLUE with more challenging tasks and a focus on compositional language understanding.
    • Stanford Question Answering Dataset (SQuAD) is a dataset for evaluating question-answering capabilities.
    • Workshop on Machine Translation (WMT) is a series of datasets and tasks for evaluating machine translation systems.

    • "HumanEval" is a well-known benchmark specifically designed to assess the accuracy of "code generation" models.
Automated metrics
  can provide a quick and scalable way to evaluate foundation model performance.

  These metrics typically measure specific aspects of the model's outputs, such as the following:
    • Perplexity (a measure of how well the model predicts the next token)
    • BLEU score (for evaluating machine translation)
    • F1 score (for evaluating classification or entity recognition tasks)

  Automated metrics can be useful for rapid iterations and fine-tuning during model development, but they often fail to capture the nuances and complexities of human language and might not align perfectly with human judgments.

3.4.b (1)
Relevant metrics
  Metrics like ROUGE, BLEU, and BERTScore provide an initial assessment of the foundation model's capabilities.
  • ROUGE
    Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of metrics used for evaluating automatic summarization and machine translation systems. It measures the "quality of a generated summary" or translation by comparing it to one or more reference summaries or translations.
  • BLEU
    Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the "quality of machine-generated text", particularly in the context of machine translation. It measures the "similarity" between a generated text and one or more reference translations, considering both precision and brevity.
  • BERTScore
    BERTScore is a metric that evaluates the "semantic similarity" between a generated text and one or more reference texts. It uses pre-trained Bidirectional Encoder Representations from Transformers (BERT) models to compute contextualized embeddings for the input texts, and then calculates the cosine similarity between them.


Deploying the Application:
The deployment phase of the generative AI lifecycle ensures that the trained model is successfully integrated into the target environment for practical use.
During this phase, careful consideration is given to factors such as system architecture, scalability, security, and user experience to ensure a seamless and efficient deployment.

Key considerations
  • Cost: Pay for the resources that you use with no minimum fees.
  • Regions: Model deployment is limited to certain AWS Regions.
  • Quotas: Ensure that you have the adequate service resources for your AWS account.
  • Security:
    • If your model is deployed in AWS infrastructure, the security responsibility is shared between the company and AWS.
    • If accessing a model outside of AWS, security considerations must be evaluated for data leaving the AWS account.
=====



Course: essentials-of-prompt-engineering
=====
3.2.a
Understanding Prompts:
Using effective prompt strategies can offer you the following benefits:
  • Enhance the model's capabilities and bolster its safety measures.
  • Equip the model with domain-specific knowledge and external tools without modifying its parameters or undergoing fine-tuning.
  • Interact with language models to fully comprehend their potential.
  • Obtain higher-quality outputs by providing higher-quality inputs.

Elements of a prompt:
  • Instructions: This is a task for the large language model to do. It provides a task description or instruction for how the model should perform.
  • Context: This is external information to guide the model.
  • Input data: This is the input for which you want a response.
  • Output indicator: This is the output type or format.
  • Negative prompting:
      Negative prompting is used to guide the model away from producing certain types of content or exhibiting specific behaviors.
      For instance, in a text generation model, negative prompts could be used to prevent the model from producing hate speech, explicit content, or biased language. By specifying what the model should avoid, negative prompting helps steer the output towards more appropriate content.

3.1.b
Modifying Prompts:
Inference parameters
  When interacting with FMs, you can often configure inference parameters to limit or influence the model response. The parameters available to you will vary based on the model that you are using. Inference parameters fit into a range of categories, with the most common being "randomness" and "diversity" and "length".

  1. Randomness and diversity
  influence the variation in generated responses by limiting the outputs to more likely outcomes or by changing the shape of the probability distribution of outputs.
    • Temperature:
        controls the randomness or creativity of the model's output.
        Temperature is set between 0 and 1.
        A lower temperature makes it more focused and predictable.
          Outputs are more conservative, repetitive, and focused on the most likely responses.
        A higher temperature makes the output more diverse and unpredictable.
          Outputs are more diverse, creative, and unpredictable, but might be less coherent or relevant.
    • Top P (or nucleus):
        Top p is a setting that controls the diversity of the text by limiting the number of words that the model can choose from based on their "probabilities".
        Top p is also set on a scale from 0 to 1.
        Low top p (for example, 0.250)
          model will only consider words that make up the top 25 percent of the total probability distribution. This can help the output be more focused and coherent, because the model is limited to choosing from the most probable words given the context.
        High top p (for example, 0.990)
          model will consider a broad range of possible words for the next word in the sequence, because it will include words that make up the top 99 percent of the total probability distribution. This can lead to more diverse and creative output, because the model has a wider pool of words to choose from.
    • Top K:
        Top k limits the number of words to the top k most "probable words", regardless of their "percent probabilities".
        Google statement - "Probable" describes something that is likely to happen or be true, while "probability" is the mathematical concept of the likelihood of an event occurring, often expressed as a number between 0 and 1.
        Low top k (for example, 10)
          model will only consider the 10 most probable words for the next word in the sequence. This can help the output be more focused and coherent, because the model is limited to choosing from the most probable words given the context.
        High top k (for example, 500)
          model will consider the 500 most probable words for the next word in the sequence, regardless of their individual probabilities. This can lead to more diverse and creative output, because the model has a larger pool of potential words to choose from.
  2. Length
    settings that control the maximum length of the generated output and specify the stop sequences that signal the end of the generation process.
    • Maximum length:
        setting determines the maximum number of tokens that the model can generate during the inference process.
    • Stop sequences:
        Stop sequences are special tokens or sequences of tokens that signal the model to stop generating further output. When the model encounters a stop sequence during the inference process, it will terminate the generation regardless of the maximum length setting. Stop sequences are particularly useful in tasks where the desired output length is variable or difficult to predict in advance. For example, in conversational artificial intelligence (AI) systems, the stop sequence could be an end-of-conversation token or a specific phrase that indicates the end of the response.

        Stop sequences can be predefined or dynamically generated based on the input or the generated output itself. In some cases, multiple stop sequences can be specified, allowing the model to stop generation upon encountering any of the defined sequences.

3.2.c
Best practices for prompting
  • Be clear and concise.
    Prompts should be straightforward and avoid ambiguity.
  • Include context if needed.
  • Use directives for the appropriate response type.
    If you want a particular output form, such as a summary, question, or poem, specify the response type directly.
    You can also limit responses by length, format, included information, excluded information, and more.
  • Consider the output in the prompt.
    Mention the requested output at the end of the prompt to keep the model focused on appropriate content.
  • Start prompts with an interrogation.
    Phrase your input as a question, beginning with words, such as who, what, where, when, why, and how.
  • Provide an example response.
    Use the expected output format as an example response in the prompt. Surround it in brackets to make it clear that it is an example.
  • Break up complex tasks.
    • Divide the task into several subtasks. If you cannot get reliable results, try splitting the task into multiple prompts.
    • Ask the model if it understood your instruction. Provide clarification based on the model's response.
    • If you don’t know how to break the task into subtasks, ask the model to think step by step.
  • Experiment and be creative.
  • Use prompt templates.
    "Prompt templates" are predefined structures or formats that can be used to provide consistent inputs to FMs. They help ensure that the prompts are phrased in a way that is easily understood by the model and can lead to more reliable and higher-quality outputs. Prompt templates often include instructions, context, examples, and placeholders for information relevant to the task at hand.
    Prompt templates provide a structure that ensures the model generates content while adhering to specific guidelines and constraints, giving the user more control over the output.

3.2.b
Prompt Engineering Techniques
• Zero shot - No examples of similar examples/shots of similar tasks; output based on it's knowledge
    Eg: Translate "hello" to French
    To optimize zero-shot prompting, consider the following tips:
      • The larger and more capable the FM, the higher the likelihood of obtaining effective results from zero-shot prompts.
      • Instruction tuning, a process of fine-tuning models to better align with human preferences, can enhance zero-shot learning capabilities. One approach to scale instruction tuning is through "reinforcement learning from human feedback (RLHF)", where the model is iteratively trained based on human evaluations of its outputs.
• Few-shot prompting - giving sample inputs and their corresponding desired outputs
    Although few-shot prompting provides a model with multiple examples, you can also use single-shot or one-shot prompting by providing just one example.
    When employing a few-shot prompting technique, consider the following tips:
      • Make sure to select examples that are representative of the task that you want the model to perform and cover a diverse range of inputs and outputs. Additionally, aim to use clear and concise examples that accurately demonstrate the desired behavior.
      • Experiment with the number of examples. The optimal number of examples to include in a few-shot prompt can vary depending on the task, the model, and the complexity of the examples themselves. Generally, providing more examples can help the model better understand the task. But too many examples might introduce noise or confusion.
• One shot - One example
    Eg: English: Cat, French: Chat; English: Dog, French?
• Two shot - Two examples
    Eg: A dog is a mammal, A cat is a mammal. What is Lion?
• Chain-of-thought (CoT) prompting
    divides intricate reasoning tasks into smaller, intermediary steps.
    This approach can be employed using either zero-shot or few-shot prompting techniques.
    CoT prompts are tailored to specific problem types.
    use the phrase "Think step by step."
    use when the task requires multiple steps or a series of logical reasoning.
    Chain-of-thought prompting can be combined with the few-shot prompting technique to break down tasks into a step-by-step process. Including examples of solved word problems can help the model to generate more informed outputs.
• Self-consistency
• Tree of thoughts (ToT)
• Retrieval Augmented Generation (RAG)
• Automatic Reasoning and Tool-use (ART)
• ReAct prompting

3.2.d
Prompt Misuses and Risks:
"Poisoning" (add malicioius data to training data)
  Poisoning refers to the intentional introduction of malicious or biased data into the training dataset of a model. This can lead to the model producing biased, offensive, or harmful outputs, either intentionally or unintentionally.
"Hijacking" and "prompt injection" (sending prompts to manipulate model behaviour)
  Hijacking and prompt injection refer to the technique of influencing the outputs of generative models by embedding specific instructions within the prompts themselves.
"Exposure" (model exposing sensitive information used in training)
  Exposure refers to the risk of exposing sensitive or confidential information to a generative model during training or inference. An FM can then inadvertently reveal this sensitive data from their training corpus, leading to potential data leaks or privacy violations.
"Prompt leaking" (leaking own System prompts)
  Prompt leaking refers to the unintentional disclosure or leakage of the prompts or inputs (regardless of whether these are protected data or not) used within a model. Prompt leaking does not necessarily expose protected data. But it can expose other data used by the model, which can reveal information of how the model works and this can be used against it.
"Jailbreaking"
  Jailbreaking refers to the practice of modifying or circumventing the constraints and safety measures implemented in a generative model or AI assistant to gain unauthorized access or functionality.
  Jailbreaking attempts involve crafting carefully constructed prompts or input sequences that aim to bypass or exploit vulnerabilities in the AI system's filtering mechanisms or constraints. The goal is to "break out" of the intended model limitations.
=====


Course: optimizing-foundation-models (Explained the whole flow using Business Use Case)
=====
large language model (LLM) are FMs that have the ability to understand and process natural language.

Retrieval-Augmented Generation (RAG) approach that allows FMs to query knowledge bases to provide accurate and up-to-date answers to customer prompts.
From dataset to vector embeddings
Vector embeddings
  Words that relate to each other will have closer embeddings.
Storing vectors
  The core function of vector databases is to compactly store billions of high-dimensional vectors representing words and entities.
  Vector databases provide ultra-fast similarity searches across these billions of vectors in real time. 

  The most common algorithms used to perform the similarity search are "k-nearest neighbors (k-NN)" or "cosine similarity".

  Amazon Web Services (AWS) offers the following viable vector database options:
  • Amazon OpenSearch Service (provisioned)
  • Amazon OpenSearch Serverless
  • pgvector extension in Amazon Relational Database Service (Amazon RDS) for PostgreSQL
  • pgvector extension in Amazon Aurora PostgreSQL-Compatible Edition
  • Amazon Kendra

Agents
  Key functions of agents
    • Intermediary operations
        Agents can act as intermediaries, facilitating communication between the generative AI model and various backend systems.
    • Actions launch
    • Feedback integration

  Eg:
    Agent 1 to modify customer plan features
    Agent 2 uses conversations to improve or update the enterprise data
    Agent 3 understands when the conversation ends and sends a customer satisfication survey

3.4.a (2)
Evaluate results
  Human evaluation
    assessing "qualitative aspects" of the model, such as the following:
      User experience
      Contextual approriateness
      Creativity and flexibility
  Benchmark datasets
    provide a "quantitative way" to evaluate generative AI models, such as:
      Accuracy
      Speed and efficiency
      Scalability
    useful for initial testing phases
    essential for comparing performance across different models or different iterations of the same model.

    LLM as a judge approach:
    The judge model(compares answer in dataset with model generated answer) calculates a grading score to assess the performance of the model. This score should take in to account metrics such as accuracy (correctness of the response), relevance (suitability of the response to the question), and comprehensiveness (depth and breadth of the response).

  Human evaluation involves "subjective" assessment by humans, while benchmark datasets provide "objective, quantitative" measures of performance.

AnyCompany use Generative AI, and will monitor the following metrics:
  Conversion rate: Increase in successful purchases for each site visit
  Average order value: Increase in the dollar amount spent for each transaction
  Customer retention rate: Increase in the percentage of returning customers

Fine-Tuning
  • Increase specificity: Adapt the model’s responses or predictions to the nuances of a specific domain or task that were not adequately covered in the initial training.
  • Improve accuracy: Enhance the model's performance on specialized tasks by training on domain-specific data, thereby reducing errors that occur due to the generic nature of foundational training.
  • Reduce biases: Address and mitigate any biases inherent in the initial training data, making the model more fair and appropriate for different applications.
  • Boost efficiency: Streamline the model’s operations within specific contexts, potentially reducing computational requirements and speeding up response times.

3.3.b
The different fine-tuning approaches
  • Instruction tuning
    This approach involves retraining the model on a new dataset that consists of prompts followed by the desired outputs.
    This is structured in a way that the model learns to follow specific instructions better. This method is particularly useful for improving the model's ability to understand and execute user commands accurately, making it highly effective for interactive applications like "virtual assistants" and "chatbots". 
  • Reinforcement learning from human feedback (RLHF)
    This approach is a fine-tuning technique where the model is initially trained using supervised learning to predict human-like responses. Then, it is further refined through a reinforcement learning process, where a reward model built from human feedback guides the model toward generating more preferable outputs.
    This method is effective in aligning the model’s outputs with human values and preferences, thereby increasing its practical utility in sensitive applications.
    RLHF refers to the improvement of the model by learning from feedback, such as ratings, preferences, demonstrations, helpfulness, or toxicity, provided by humans. RLHF is used during the pretraining phase of the model but can also be used to fine-tune the model.
  • Adapting models for specific domains
    This approach involves fine-tuning the model on a corpus of text or data that is specific to a particular industry or sector.
    An example of this would be legal documents for a legal AI or medical records for a healthcare AI. This specificity enables the model to perform with a higher degree of relevance and accuracy in domain-specific tasks, providing more useful and context-aware responses.
  • Transfer learning
    This approach is a method where a model developed for one task is reused as the starting point for a model on a second task.
    For foundational models, this often means taking a model that has been trained on a vast, general dataset, then fine-tuning it on a smaller, specific dataset. This method is highly efficient in using learned features and knowledge from the general training phase and applying them to a narrower scope with less additional training required.
  • Continuous pretraining
    This approach involves extending the training phase of a pre-trained model by continuously feeding it new and emerging data.
    This approach is used to keep the model updated with the latest information, vocabulary, trends, or research findings, ensuring its outputs remain relevant and accurate over time.

3.3.c
Preparing the data for the fine-tuning step
The goals during the initial training phase are as follows:
  • Extensive coverage: Ensuring the dataset covers a broad spectrum of knowledge to give the model a robust foundational understanding
  • Diversity: Including varied types of data from numerous sources to equip the model with the ability to handle a wide array of tasks
  • Generalization: Focusing on building a model that can generalize across different tasks and domains without specific tailoring

  The data needs thorough cleaning and possibly anonymization to ensure privacy and compliance with regulations.

The data preparation for fine-tuning is distinct from initial training due to the following reasons:
  • Specificity: The dataset for fine-tuning is much more focused, containing examples that are directly relevant to the specific tasks or problems the model needs to solve.
  • High relevance: Data must be highly relevant to the desired outputs. Examples include legal documents for a legal AI or customer service interactions for a customer support AI.
  • Quality over quantity: Although the initial training requires massive amounts of data, fine-tuning can often achieve significant improvements with much smaller, but well-curated datasets.

Key steps in fine-tuning data preparation:
  • Data curation
    Although it is a continuation, this involves a more rigorous selection process to ensure every piece of data is highly relevant. This step also ensures the data contributes to the model's learning in the specific context.
  • Labeling
    In fine-tuning, the accuracy and relevance of labels are paramount. They guide the model's adjustments to specialize in the target domain.
  • Governance and compliance
    Considering fine-tuning often uses more specialized data, ensuring data governance and compliance with industry-specific regulations is critical.
  • Representativeness and bias checking
    It is essential to ensure that the fine-tuning dataset does not introduce or perpetuate biases that could skew the model's performance in undesirable ways.
  • Feedback integration
    For methods like RLHF, incorporating user or expert feedback directly into the training process is crucial. This is more nuanced and interactive than the initial training phase.
In the fine-tuning process, labeling the data with accurate and relevant labels is crucial for guiding the model's adjustments to specialize in the target domain.

3.4.b (2)
Model evaluation
  • Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
    ROUGE is a set of metrics used to evaluate automatic summarization of texts, in addition to machine translation quality in NLP. The main idea behind ROUGE is to count the number of overlapping units. This includes words, N-grams, or sentence fragments between the computer-generated output and a set of reference (human-created) texts.

    The following are two ways to use the ROUGE metric:

    • ROUGE-N: This metric measures the overlap of n-grams between the generated text and the reference text. For example, ROUGE-1 refers to the overlap of unigrams, ROUGE-2 refers to bigrams, and so on. This metric primarily assesses the fluency of the text and the extent to which it includes key ideas from the reference.
    • ROUGE-L: This metric uses the longest common subsequence between the generated text and the reference texts. It is particularly good at evaluating the coherence and order of the narrative in the outputs.
    ROUGE is widely used because it is not complex. It is interpretable, and correlates reasonably well with human judgment, especially when evaluating the recall aspect of summaries. The evaluations assess how much of the important information in the source texts is captured by the generated summaries.
  • Bilingual Evaluation Understudy (BLEU)
    BLEU is a metric used to evaluate the quality of text that has been machine-translated from one natural language to another. Quality is calculated by comparing the machine-generated text to one or more high-quality human translations. BLEU measures the precision of N-grams in the machine-generated text that appears in the reference texts and applies a penalty for overly short translations (brevity penalty).

    Unlike ROUGE, which focuses on "recall", BLEU is fundamentally a "precision" metric. It checks how many words or phrases in the machine translation appear in the reference translations. BLEU evaluates the quality at the level of the sentence, typically using a combination of unigrams, bigrams, trigrams, and quadrigrams. A brevity penalty discourages overly concise translations that might influence the precision score.

    BLEU is popular in the field of machine translation for its ease of use and effectiveness at a broad scale. However, it has limitations in assessing the fluency and grammaticality of the output.
  • BERTScore
    BERTScore uses the pretrained contextual embeddings from models like BERT to evaluate the quality of text-generation tasks. BERTScore computes the cosine similarity between the contextual embeddings of words in the candidate and the reference texts. This is unlike traditional metrics that rely on exact matches of N-grams or words.

    Because BERTScore evaluates the "semantic similarity" rather than relying on exact lexical matches, it is capable of capturing meaning in a more nuanced manner. BERTScore is less prone to some of the pitfalls of BLEU and ROUGE. An example of this is their sensitivity to minor paraphrasing or synonym usage that does not affect the overall meaning conveyed by the text.

    BERTScore is increasingly used alongside traditional metrics like BLEU and ROUGE for a more "comprehensive assessment" of language generation models. This is especially true in cases where capturing the deeper semantic meaning of the text is important.


AnyCompany use Generative AI, and will monitor the following metrics:
  Conversion rate: Increase in successful purchases for each site visit
    metric ROUGE used here
  Average order value: Increase in the dollar amount spent for each transaction
    metric BLEU used here
  Customer retention rate: Increase in the percentage of returning customers
    metric BERTScore used here

ROUGE and BLEU evalutes the dynamic product descriptions generated
BERTScore evaluates the personalized product displayed
=====


Course: Responsible Artificial Intelligence Practices (Domain 4)
=====
Responsible AI refers to "practices" and "principles" that ensure that AI systems are "transparent" and "trustworthy" while "mitigating potential risks and negative outcomes".  
These responsible standards should be considered throughout the entire lifecycle of an AI application. This includes the initial design, development, deployment, monitoring, and evaluation phases.  

To operate AI responsibly, companies should proactively ensure the following about their system:  
  - It is fully transparent and accountable, with monitoring and oversight mechanisms in place.
  - It is managed by a leadership team accountable for responsible AI strategies.
  - It is developed by teams with expertise in responsible AI principles and practices.
  - It is built following responsible AI guidelines.

Responsible AI is not exclusive to any one form of AI. It should be considered when you are building "traditional" or "generative" AI systems.

Traditional AI  
  Traditional machine learning models perform tasks based on the data you provide. They can make predictions such as ranking, sentiment analysis, image classification, and more.
  However, each model can perform only one task. And to successfully do it, the model needs to be carefully trained on the data. As they train, they analyze the data and look for patterns. Then these models make a prediction based on these patterns. 

  Some examples of traditional AI include recommendation engines, gaming, and voice assistance. 
Generative AI
  Generative artificial intelligence (generative AI) runs on "foundation models (FMs)". These models are pre-trained on massive amounts of general domain data that is beyond your own data.
  They can perform multiple tasks. Based on user input, usually in the form of text called a prompt, the model actually generates content. This content comes from learning patterns and relationships that empower the model to predict the desired outcome. 

  Some examples of generative AI include chatbots, code generation, and text and image generation.

Generative AI offers business value
  • Creativity: Create new content and ideas, including conversations, stories, images, videos, and music.
  • Productivity: Radically improve productivity across all lines of business, use cases, and industries.
  • Connectivity: Connect and engage with customers and across organizations in new ways.


Responsible AI Challenges in Traditional AI and Generative AI
Biases in AI systems
• Accuracy of models
    models can make predictions or generate content based only on the data they are trained on. If they are not trained properly, you will get inaccurate results.
    Therefore, it is important to address bias and variance in your model.
    Bias
      Bias in a model means that the model is missing important features of the datasets. This means that the data is too basic. Bias is measured by the difference between the "expected predictions" of the model and the "true values we are trying to predict".
      If the difference is narrow, then the model has low bias. If the difference is wide, then the model has a high bias.
      When a model has a high bias, it is "underfitted". Underfitted means that the model is not capturing enough difference in the features of the data, and therefore, the model performs poorly on the training data.
    Variance
      Variance refers to the "model's sensitivity to fluctuations or noise in the training data". The problem is that the model might consider noise in the data to be important in the output. When variance is high, the model becomes so familiar with the training data that it can make predictions with high accuracy. This is because it is capturing all the features of the data.
      However, when you introduce new data to the model, the model's accuracy drops. This is because the new data can have different features that the model is not trained on. This introduces the problem of "overfitting". Overfitting is when model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.
• Bias-variance trade-off
    Bias-variance tradeoff is when you optimize your model with the right balance between bias and variance. This means that you need to optimize your model so that it is not underfitted or overfitted. The goal is to achieve a trained model with the lowest bias and lowest variance tradeoff for a given data set.
    Can be overcome by:
      • Cross validation
      • Increase data
      • Regularization
      • Simpler models
      • Dimension reduction (Principal component analysis)
      • Stop training early
      <in detail in course>

Challenges of generative AI
Toxicity (generate inappropriate content)
  Toxicity is the possibility of generating content (whether it be text, images, or other modalities) that is offensive, disturbing, or otherwise inappropriate.
Hallucinations (inaccurate responses)
  Hallucinations are assertions or claims that sound plausible but are verifiably incorrect.
  Example: "Tell me about some papers by" a particular author. The model is not actually searching for legitimate citations but generating ones from the distribution of words associated with that author.
Intellectual property
  tendency to occasionally produce text or code passages that were verbatim of parts of their "training data", resulting in privacy and other concerns
Plagiarism and cheating
  Being used to write college essays(where content is being graded or evaluated), writing samples for job applications, and other forms of cheating or illicit copying.
Disruption of the nature of work
  concern that some professions might be replaced or seriously disrupted by the technology.

Core Dimensions of Responsible AI
<Responsible AI core dimensions.png>
Fairness
    AI systems promote inclusion, prevent discrimination, uphold responsible values and legal norms, and build trust with society.
    to create systems suitable and beneficial for all.
Explainability
    clearly explain or provide justification for its internal mechanisms and decisions so that it is understandable to humans.
    Humans must understand how models are making decisions and address any issues of bias, trust, or fairness.
Privacy and security
    data that is protected from theft and exposure.
    at a privacy level, individuals control when and if their data can be used.
    At the security level, it verifies that no unauthorized systems or unauthorized users will have access to the individual’s data.
Transparency  
    Transparency communicates information about an AI system so stakeholders can make informed choices about their use of the system. Some of this information includes development processes, system capabilities, and limitations.  
    It provides individuals, organizations, and stakeholders access to assess the fairness, robustness, and explainability of AI systems. They can identify and mitigate potential biases, reinforce responsible standards, and foster trust in the technology.  
Veracity and robustness  
    AI system operates reliably, even with unexpected situations, uncertainty, and errors.  
    resilient to changes in input parameters, data distributions, and external circumstances.  
    retain reliability, accuracy, and safety in uncertain environments.  
Governance  
    Governance is a set of processes that are used to "define, implement, and enforce responsible AI practices" within an organization.  
    Governance addresses various responsible, legal, or societal problems that generative AI might invite.  
    For example, governance policies can help to protect the rights of individuals to intellectual property.  
Safety  
    development of algorithms, models, and systems in such a way that they are responsible, safe, and beneficial for individuals and society as a whole.  
    designed and tested to avoid causing unintended harm to humans or the environment. Things like bias, misuse, and uncontrolled impacts need to be proactively considered.  
Controllability  
    ability to monitor and guide an AI system's behavior to align with human values and intent. It involves developing architectures that are controllable, so that any unintended issues can be managed and addressed.  
    helps mitigate risks, promote fairness and transparency, and ensure that AI systems benefit society as a whole. 

Business benefits of responsible AI
Increased trust and reputation  
    Customers are more likely to interact with AI applications, if they believe the system is fair and safe. This enhances their reputation and brand value.
Regulatory compliance  
    As AI regulations emerge, companies with robust ethical AI frameworks are better positioned to comply with guidelines on data privacy, fairness, accountability, and transparency.
Mitigating risks  
    Responsible AI practices help mitigate risks such as bias, privacy violations, security breaches, and unintended negative impacts on society. This reduces legal liabilities and financial costs.
Competitive advantage  
    Companies that prioritize responsible AI can differentiate themselves from competitors and gain a competitive edge, especially as consumer awareness of AI ethics grows.
Improved decision-making  
    AI systems built with fairness, accountability, and transparency in mind are more reliable and less likely to produce biased or flawed outputs, which leads to better data-driven decisions.
Improved products and business  
    Responsible AI encourages a diverse and inclusive approach to AI development. Because it draws on varied perspectives and experiences, it can drive more creative and innovative solutions.


Amazon Services and Tools for Responsible AI
• Amazon SageMaker is a fully managed ML service.
• Amazon Bedrock is a fully managed service that makes available high-performing FMs from leading AI startups and Amazon for our use through a unified API.
  Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. 
  With the serverless experience of Amazon Bedrock, you can privately customize FMs with your own data and securely integrate and deploy them into your applications by using AWS tools without having to manage any infrastructure.

Reviewing Amazon service tools for responsible AI
• Foundation model evaluation
    "Model evaluation on Amazon Bedrock" - can evaluate, compare, and select the best foundation model for your use case in just a few clicks.
    Amazon Bedrock offers a choice of automatic evaluation and human evaluation.
        • Automatic evaluation offers predefined metrics such as accuracy, robustness, and toxicity. 
        • Human evaluation offers subjective or custom metrics such as friendliness, style, and alignment to brand voice. For human evaluation, you can use your in-house employees or an AWS-managed team as reviewers.
    "SageMaker Clarify" - can automatically evaluate FMs for metrics such as accuracy, robustness, and toxicity to support your responsible AI initiative.
• Safeguards for generative AI
    "Amazon Bedrock Guardrails" - implement safeguards
        Guardrails helps control the interaction between users and FMs by filtering undesirable and harmful content, redacting personally identifiable information (PII), and enhancing content safety and privacy in generative AI applications.
        Additionally, you can continuously monitor and analyze user inputs and FM responses that can violate customer-defined policies in the guardrails.
            • Consistent level of AI safety
            • Block undesirable topics
            • Filter harmful content
            • Redact PII to protect user privacy
• Bias detection
    "Amazon SageMaker Clarify" - helps identify potential bias in machine learning models and datasets without the need for extensive coding.
    "Amazon SageMaker Data Wrangler" - to "balance your data" in cases of any imbalances.
        Offers three balancing operators: random undersampling, random oversampling, and Synthetic Minority Oversampling Technique (SMOTE) to rebalance data in your unbalanced datasets.
• Model prediction explanation
    "SageMaker Clarify" is integrated with "Amazon SageMaker Experiments" to provide scores detailing which features contributed the most to your model prediction on a particular input for tabular, natural language processing (NLP), and computer vision models.
        For tabular datasets, SageMaker Clarify can also output an aggregated feature importance chart that provides insights into the overall prediction process of the model.
        These details can help determine if a particular model input has more influence than expected on overall model behavior.
• Monitoring and human reviews
    "Amazon SageMaker Model Monitor" - monitors the quality of SageMaker machine learning models in production.
        You can set up continuous monitoring with a real-time endpoint (or a batch transform job that runs regularly), or on-schedule monitoring for asynchronous batch transform jobs.
        You can set alerts that notify you when there are deviations in the model quality.
    "Amazon Augmented AI (Amazon A2I)" - helps build the workflows required for human review of "ML predictions".
        Brings human review to all developers and removes the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers.
        low confidence/random predictions sent for human review.
        Human reviewers can review data extracted from "Amazon Textract forms", moderate "images in Amazon Rekognition", or review data using a "custom workflow".
• Governance improvement
    "Amazon SageMaker Role Manager" - administrators can define minimum permissions in minutes. 
    "Amazon SageMaker Model Cards" - to capture, retrieve, and share essential model information, such as intended uses, risk ratings, and training details, from conception to deployment. 
    "Amazon SageMaker Model Dashboard" - to keep your team informed on model behavior in production, all in one place.
• Providing transparency
    "AWS AI Service Cards" - help you better understand AWS AI services.
        Form of responsible AI documentation that provides a single place to find information on the intended use cases and limitations, responsible AI design choices, and deployment and performance optimization best practices for AWS AI services.
        They are part of a comprehensive development process to build AWS services in a responsible way that addresses the core dimensions of responsible AI.
        Each AI Service Card contains four sections that cover the following:
            • Basic concepts to help customers better understand the service or service features
            • Intended use cases and limitations
            • Responsible AI design considerations
            • Guidance on deployment and performance optimization

4.1.c
Responsible Considerations to Select a Model
  use "Model evaluation on Amazon Bedrock" or "SageMaker Clarify" to evaluate models for accuracy, robustness, toxicity, or nuanced content that requires human judgement.

  Define application use case narrowly
    Example: Defining application use case narrowly for traditional AI
      Face recognition is not a use case; it is a technology. The way your model applies that technology is a use case.
      For example, a gallery retrieval application might be used to help find missing persons.
      Other face recognition applications are Celebrity recognition and Virtual proctoring.
    Example: Defining application use case narrowly for generative AI
      In an AI application to catalog a product, you would want a broad demographic target audience so that it is available for all of your customers. 
      In an AI application to persuade to buy, you would want a narrow target audience to capture a specific group of people. For example, you might want to target an audience that lives on the coast to buy accessories for docking boats.
  Choosing a model based on performance
    • Level of customization – The ability to change a model’s output with new data ranging from prompt-based approaches to full model retraining
    • Model size – The amount of information the model has learned as defined by parameter count
    • Inference options – From self-managed deployment to API calls
    • Licensing agreements – Some agreements can restrict or prohibit commercial use
    • Context windows – The amount of information that can fit in a single prompt
    • Latency – The amount of time it takes for a model to generate an output

    Performance is a function of the model and a test dataset, not just the model. So, when you are assessing a model, you need to determine how well a model performs on a particular dataset.
    This means that you need to consider two development trajectories: the development trajectory of the model and the development trajectory of the datasets.
  Choosing a model based on sustainability concerns
    Sustainability here is - ability of AI systems to be developed and deployed in a way that is socially, environmentally, and economically sustainable over the long term.

    Responsible agency considerations for selecting a model
      AI system's capacity to make good judgments and act in a socially responsible manner.
      The following are key aspects of moral agency for AI:
      • Value alignment
          Value alignment is being able to understand, evaluate, and make decisions based on moral principles rather than pure utility maximization. This requires value alignment between the AI system's goals and values and the responsible human values. 
      • Responsible reasoning skills
          Responsible reasoning skills is being able to logically think through moral dilemmas and weigh various responsible considerations when making decisions. The AI needs logic and reasoning capabilities to apply responsible principles to novel situations. 
          The AI system should have the capacity to engage in responsible reasoning and understand moral concepts, principles, and frameworks. It should be able to apply them in context to specific situations.
      • Appropriate level of autonomy
          The AI system should have the appropriate level of autonomy, with clear boundaries and mechanisms for human oversight and intervention, particularly in high-stakes or sensitive domains.
      • Transparency and accountability
          The AI system should be transparent about its decision-making process. It should allow external oversight and accountability to ensure its actions are responsibly justified.
    Environmental considerations for selecting a model
      Key environmental challenges and solutions:
      • Energy consumption
      • Resources utilization
      • Environmental impact assessment
      (in detail in course)
    Economic considerations for selecting a model
      potential benefits and costs of AI technologies and the impact on jobs and the economy.

Responsible Preparation for Datasets
  use "SageMaker Clarify" and "SageMaker Data Wrangler" to help balance your datasets.

  Balancing datasets
    Inclusive and diverse data collection
      Data collection should accurately reflect the diverse perspectives and experiences required for the use case of the AI system. This includes a diverse range of sources, viewpoints, and demographics.
      Collection of data for people, scientific research, geography, weather, products, and other topics should be collected with a focus on the diverse range for each topic.
    Data curation
      • Data preprocessing
          Preprocess the data to ensure it is accurate, complete, and unbiased. Techniques such as data cleaning, normalization, and feature selection can help to eliminate biases in the dataset.
      • Data augmentation
          Use data augmentation techniques to generate new instances of underrepresented groups. This can help to balance the dataset and prevent biases towards more represented groups.
      • Regular auditing
          Regularly audit the dataset to ensure it remains balanced and fair. Check for biases and take corrective actions if necessary.

  Balance your data for the intended use case

4.2.a
Transparent and Explainable Models
  To promote trust and accountability in an AI system, there should be transparency and explainability in the model.
  
  Transparency helps to understand HOW a model makes decisions.
  This helps to provide accountability and builds trust in the AI system. Transparency also makes auditing a system easier.

  Explainability helps to understand WHY the model made the decision that it made. It gives insight into the limitations of a model.
  This helps developers with debugging and troubleshooting the model. It also allows users to make informed decisions on how to use the model. 

  black box models - Models that lack transparency and explainability
  These models use complex algorithms and numerous layers of neural networks to make predictions, but they do not provide insight into their internal workings.

  Transparent and explainable models have several advantages over black box models.
    • Increased trust
      particularly important in high-stakes applications, such as healthcare, financial services, and transportation, where it is crucial to understand the reasoning behind the models' decisions. 
    • Easier to debug and optimize for improvements
      As internal working is transparent
    • Better understanding of the data and the model's decision-making process
      particularly important in applications where explainability is a key consideration, such as in healthcare, where patients need to understand why a particular treatment was recommended.

  Solutions for transparent and explainable models
    • Explainability frameworks
        There are several explainability frameworks available, such as SHapley Value Added (SHAP), Layout-Independent Matrix Factorization (LIME), and Counterfactual Explanations, that can help summarize and interpret the decisions made by AI systems. These frameworks can provide insights into the factors that influenced a particular decision and help assess the fairness and consistency of the AI system.
    • Transparent documentation
        Maintain clear and comprehensive documentation of the AI system's architecture, data sources, training processes, and underlying assumptions, which can be made available to relevant stakeholders and auditors.
        This can include user guides, technical documentation, and visualizations that help users understand the underlying algorithms and their inputs and outputs.
    • Monitoring and auditing
        AI systems should be monitored and audited to ensure that they are functioning as intended and not exhibiting bias or discriminatory behavior. This can include regular testing and oversight by humans and automated tools to identify unusual patterns or decisions.
    • Human oversight and involvement
        Incorporate human oversight and involvement in critical decision-making processes where humans can review and validate the AI system's outputs and decisions, especially in high-stakes situations.
    • Counterfactual explanations
        Provide counterfactual explanations that show how the output would change if certain input features were different to help users understand the model's behavior and reasoning.
    • User interface explanations
        Design user interfaces that provide clear and understandable explanations of the AI system's outputs, rationale, and limitations to end-users, so they can make informed decisions.

  Risks of transparent and explainable models
    • Increasing the complexity of the development and maintenance of the model can increase the costs.
    • Creating vulnerabilities of the model, data, and algorithms can be exploited by bad actors.
    • Presenting unrealistic expectations that the model is perfectly transparent and explainable. In some situations, this may not be achievable or even intended.
    • Providing too much information that can create privacy and security concerns. It could also lead to compromising the competitive edge of the model.

  4.2.b
  AWS tools for transparent and explainability
  AWS tools for transparency
    • "AWS AI Service Cards" - Amazon provides transparent documentation on Amazon services that help you build your AI services.
      AI Service Cards are a resource to increase transparency and help customers better understand AWS AI services, including how to use them in a responsible way. AI service cards are a form of responsible AI documentation that provides customers with a single place to find information on the intended use cases and limitations, responsible AI design choices, and the deployment and operation best practices for our AI services.
    • "Amazon SageMaker Model Cards" - you can catalog and provide documentation on models that you create or develop yourself.
      Use SageMaker Model Cards to document critical details about your ML models in a single place for streamlined governance and reporting.
      Catalog details include information such as the intended use and risk rating of a model, training details and metrics, evaluation results and observations, and additional callouts such as considerations, recommendations, and custom information. 
  AWS tools for explainability
    • "SageMaker Clarify"
      SageMaker Clarify is integrated with "SageMaker Experiments" to provide scores detailing "which features contributed the most to your model prediction" on a particular input for tabular, NLP, and computer vision models. For tabular datasets, SageMaker Clarify can also output an aggregated feature importance chart which provides insights into the overall prediction process of the model. These details can help determine if a particular model input has more influence than expected on overall model behavior.
    • "SageMaker Autopilot" (how ML models make predictions)
      Amazon SageMaker Autopilot uses tools provided by "SageMaker Clarify" to help provide insights into "how ML models make predictions". These tools can help ML engineers, product managers, and other internal stakeholders "understand model characteristics". To trust and interpret decisions made on model predictions, both consumers and regulators rely on transparency in machine learning.

4.2.c
Model Trade-Offs
  Interpretability trade-offs
    Interpretability - feature of model transparency. degree to which a human can understand the cause of a decision.
      access into a system so that a human can interpret the model’s output based on the weights and features.
    Explainability - how to take an ML model and explain the behavior in human terms.
      With complex models (for example, black boxes), you cannot fully understand how and why the inner mechanics impact the prediction.
      However, through model agnostic methods (for example, partial dependence plots, SHAP dependence plots, or surrogate models) you can discover meaning between input data attributions and model outputs. With that understanding, you can explain the nature and behavior of the AI/ML model.
      <Interpretability%20diagram.jpg>

    If a business wants "high model transparency" and wants to "understand exactly why and how" the model is generating predictions, then they need a model that offers interpretability. However, "high interpretability typically comes at the cost of performance".
    (as seen in the diagram)
    If a company wants to achieve "high performance but still wants to have a general understanding" of the model behavior, model "explainability" starts to play a larger role.
  Safety and transparency trade-offs
    Model Safety - ability of an AI system to avoid causing harm in its interactions with the world.
      This includes avoiding social harm, such as bias in decision-making algorithms, and avoiding privacy and security vulnerability exposures.
    • Accuracy
        Complex models like large neural networks tend to be more accurate but less interpretable than simpler linear models, which are more transparent.
    • Privacy
        Privacy-preserving techniques like differential privacy can improve safety but make models harder to inspect. This can make models less transparent.
    • Safety
        Constraining or filtering model outputs for safety can reduce transparency into the original model reasoning. 
    • Security
        Highly secured air-gapped train models (models that are trained on networks that are private and do not have access to external data) might be less open to external auditing. 
  Model controllability
    Model control - A controllable model is one where you can influence the model's predictions and behavior by changing aspects of the training data. Higher controllability provides more transparency into the model and allows correcting undesired biases and outputs.
    Model controllability is measured by how much control you have over the model by changing the input data. Models that are more controllable are easier to steer towards desired behaviors. This is important for fairness because you want to be able to understand and control bias in the model. Controllability of a model is also important for transparency and debugging in a model. 
    Controllability depends on the model architecture. Linear models tend to be more controllable than complex neural models. You can test for controllability by evaluating if manipulating the data, such as adding or removing examples, causes expected changes in the model's outputs and predictions. Controllability can be improved through data augmentation techniques and by adding constraints to the model training process. 

  Summary
    • A model that provides transparency into a system so a human can explain the model’s output based on the weights and features is an example of "interpretability" in a model.
    • A model that uses model agnostic methods to explain the behavior of the model in human terms is an example of "explainability" in a model.
    • A model that avoids causing harm in its interactions with the world is an example of "safety" in a model.
    • A model that you can influence the predictions and behavior by changing aspects of the training data is an example of "controllability" in a model.

4.2.d
Principles of Human-Centered Design for Explainable AI (XAI)
  Human-centered design (HCD) - approach to creating products and services that are intuitive, easy to use, and meet the needs of the people who will be using them.
  In explainable AI, HCD helps ensure that the explanations and interfaces provided are clear, understandable, and useful to the people they are intended to serve.
  Key principles of human-centered design for explainable AI:
    • Design for amplified decision-making.
      supports decision-makers in high-stakes situations.
      maximize the benefits of using technology while minimizing potential risks and errors, especially risks and errors that can occur when humans make decisions under stress or in high-pressure environments.
      can help to mitigate sensitive errors.
      Key aspects:
        • Clarity
            information is presented in a way that is easy to understand and interpret without introducing biases or misunderstandings.
        • Simplicity
            minimizes the amount of information that needs to be processed by the user while still providing all the necessary information to make a decision.
        • Usability
            easy to use and navigate regardless of the user's level of expertise or technical skills.
        • Reflexivity
            prompts users to reflect on their decision-making process and encourages them to take responsibility for their choices.
        • Accountability
            attaches consequences to the decisions made using amplified technology so the users are held responsible for their actions.
    • Design for unbiased decision-making.
      design of decision-making processes, systems, and tools is free from biases that can influence the outcomes.
      help promote fairness and efficient use of resources.
      Involves the following steps:
        • Identify and assess potential biases.
        • Design decision-making processes and tools that are transparent and fair.
        • Train decision-makers to recognize and mitigate biases.
      Key aspects:
        • Transparency
            clear and accessible to all stakeholders. These processes should provide easy scrutiny and identification of potential biases. This can involve using data visualization techniques to make complex information more accessible and intuitive and providing clear explanations of the decision-making process and its implications. 
        • Fairness
            minimize unfairness and discrimination. They should help to ensure that all stakeholders have an equal opportunity to participate and influence the outcomes. This can involve designing decision-making processes that are inclusive of diverse perspectives and experiences. It also involves avoiding the use of biased criteria or metrics that might perpetuate stereotypes or biases. 
        • Training
            Decision-makers, including policymakers, judges, and business leaders, need to be trained to recognize and mitigate biases. This can involve providing training to help decision-makers develop strategies for managing and overcoming biases.
    • Design for human and AI learning.
      aims to create learning environments and tools that are beneficial and effective for both humans and AI.
      It encompasses a range of strategies and approaches that take into account the unique strengths and limitations of each learner and the goals and purposes of the learning experience.
      Key aspects:
        • Cognitive apprenticeship
            process in which humans learn new skills and knowledge by observing and interacting with more skilled and knowledgeable individuals, such as teachers or mentors.
            In AI learning, this involves creating learning environments where AI systems learn from human instructors and experts and gain experience and expertise through simulated or real-world scenarios.
        • Personalization
            process of tailor-making learning experiences and tools to meet the specific needs and preferences of individual learners.
            By using data analytics and ML algorithms, developers can create personalized learning recommendations and algorithms that adapt to the unique learning style and needs of each learner. 
        • User-centered design
            designing learning environments and tools that are intuitive and accessible to a wide range of learners, including those with disabilities or language barriers.
            By prioritizing user experience and usability, designers can ensure that learning environments are effective and engaging for all users.
  Involve users throughout the design process.
  Prioritize user understanding and trust.
  Tailor explanations to the user's needs and expertise.

  Reinforcement learning from human feedback (RLHF)
    an ML technique that uses human feedback to optimize ML models to self-learn more efficiently.
    Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, which makes their outcomes more accurate.
    used in both traditional AI and generative AI applications. 
    Benefits of RLHF:
      • Enhances AI performance
      • Supplies complex training parameters
      • Increases user satisfaction
    "Amazon SageMaker Ground Truth" - humans involved for making high value data sets
      offers the most comprehensive set of human-in-the-loop capabilities for incorporating human feedback across the ML lifecycle to improve model accuracy and relevancy.
      includes a data annotator for RLHF capabilities. You can give direct feedback and guidance on output that a model has generated by ranking, classifying, or doing both for its responses for RL outcomes. The data, referred to as comparison and ranking data, is effectively a reward model or reward function that is then used to train the model. You can use comparison and ranking data to customize an existing model for your use case or to fine-tune a model that you build from scratch.
=====

Security and Privacy Considerations for AI Systems
  5.1.d
  Security considerations
    • Threat detection
      • Identify and monitor for potential security threats, such as malicious actors attempting to exploit vulnerabilities in AI systems or using generative AI for malicious purposes. The following are some examples:
        • Generating fake content
        • Manipulating data
        • Automating attacks
      • You can assist threat detection by developing and deploying AI-powered threat detection systems. You can analyze network traffic, user behavior, and other data sources to detect and respond to potential threats.
    • Vulnerability management
      • Identify and address vulnerabilities in AI and generative AI systems, including software bugs, model weaknesses, and potential attack vectors (for example, malware, viruses, and email attachments).
      • Regularly conduct security assessments, "penetration testing" (attempt to find and exploit vulnerabilities), and code reviews to uncover and address vulnerabilities.
      • Implement robust "patch management" and update processes to ensure that AI systems are kept up to date and secure.
    • Infrastructure protection
      • Secure the underlying infrastructure that supports AI and generative AI systems, such as Cloud computing platforms, Edge devices and Data stores.
      • Implement strong access controls, network segmentation, encryption, and other security measures to protect the infrastructure from unauthorized access and attacks.
      • Ensure that the AI infrastructure is resilient and can withstand failures, attacks, or other disruptions.
    • Prompt injection
        adversaries attempt to manipulate the input prompts of generative AI models to generate malicious or undesirable content.
      • Employ techniques, such as "prompt filtering", "sanitization", and validation, to ensure that the input prompts are safe and do not contain malicious content.
      • Develop robust models and training procedures that are resistant to prompt injection attacks.
    • Data encryption
      • Implement strong encryption mechanisms to secure both "data at rest" and "data in transit".
      • Ensure that the encryption keys are properly managed and protected from unauthorized access.

  The Open Web Application Security Project (OWASP) Top 10 for LLMs
    industry standard list of the top 10 vulnerabilities that can impact a generative AI LLM system
    1. Prompt injection: Malicious user inputs that can manipulate the behavior of a language model
    2. Insecure output handling: Failure to properly sanitize or validate model outputs, leading to security vulnerabilities
    3. Training data poisoning: Introducing malicious data into a model's training set, causing it to learn harmful behaviors
    4. Model denial of service: Techniques that exploit vulnerabilities in a model's architecture to disrupt its availability
    5. Supply chain vulnerabilities: Weaknesses in the software, hardware, or services used to build or deploy a model
    6. Sensitive information disclosure: Leakage of sensitive data through model outputs or other unintended channels
    7. Insecure plugin design: Flaws in the design or implementation of optional model components that can be exploited
    8. Excessive agency: Granting a model too much autonomy or capability, leading to unintended and potentially harmful actions
    9. Overreliance: Over-dependence on a model's capabilities, leading to over-trust and failure to properly audit its outputs
    10. Model theft: Unauthorized access or copying of a model's parameters or architecture, allowing for its reuse or misuse

5.1.a
AWS Services and Features for Securing AI Systems
  Reasons for securing an AI system
    • AI models process sensitive data
    • AI Systems can be vulnerable to adversarial attacks
    • AI systems are increasingly integration into critical applications and decision-making processes
  The AWS Shared Responsibility Model
    The customer assumes responsibility and management of the guest operating system. This includes updates, security patches, and other associated application software, in addition to the configuration of the AWS provided security group firewall. 
    A customer's responsibility will be determined by the AWS Cloud services that a customer selects. This determines the amount of configuration work the customer must perform as part of their security responsibilities.
    Responsibility for "security IN the cloud".

    AWS operates, manages, and controls the host operating system and virtualization layer down to the physical security of the facilities in which the service operates.
    Responsibility for "security OF the cloud".
    <SharedResponsibilityModel-Final.jpg>
  AWS services for securing AI systems
    Defense in depth security
      (discussed previously)
      Four foundational AWS security services recommended for any workload, any customer, and any industry.
        • AWS Security Hub - incident response
            provides customers with a single dashboard to view all security findings, and to create and run automated playbooks.
        • AWS KMS - data protection
            encrypts data and gives customers the choice and control of using AWS managed keys or customer-managed keys to protect their data.
        • Amazon GuardDuty - threat detection
            threat detection service that monitors for suspicious activity and unauthorized behavior to protect AWS accounts, workloads, and data.
        • AWS Shield Advanced - network and application protection
            helps protect workloads against Distributed Denial of Service (DDoS) events.
            includes AWS WAF and AWS Firewall Manager.
    AWS security services
      • Identify sensitive data before training models - Amazon Macie
          "Amazon Macie" uses ML to automate sensitive data discovery at scale.
      • Manage identities and access to AWS services and resources - IAM
          "IAM", also centrally manage fine-grained permissions, and analyze access to refine permissions across AWS.
      • Limit access to your data, models, and outputs - AWS IAM Identity Center, IAM Access Analyzer, AWS Verified Access, Amazon Verified Permissions and Amazon SageMaker Role Manager
          Apply a policy of least privilege to training data, models, and applications using "AWS IAM Identity Center" (formerly SSO) and "IAM Access Analyzer".
          Explore further zero trust capabilities to add fine-grained access controls with AWS Verified Access and Amazon Verified Permissions. 
          Use "AWS Verified Access" to further eliminate the costs, complexity and performance issues related to virtual private networks (VPNs). 
          use "Amazon SageMaker Role Manager" to build and manage persona-based IAM roles for common ML needs.
          Note:
            "AWS Verified Access" focuses on secure remote access to corporate applications and resources without a VPN.
            "Amazon Verified Permissions" is a fine-grained permissions management and authorization service for custom applications built by you. 
      • Protect data from exfiltration (data theft) and manipulation
          Network Firewall, Amazon VPC and it's policies, AWS PrivateLink
      • Protect AI workloads with intelligent threat detection
          Amazon GuardDuty, Amazon Inspector and Amazon Detective
      • Automate incident response and compliance
          AWS Security Hub, AWS Config, AWS Audit Manager and AWS Artifact
      • Defend your generative AI web applications and data
          AWS Shield Advanced, AWS Firewall Manager, and AWS WAF and it's AWS WAF Bot Control
      (in detail in course)

5.1.b
Understanding Data and Model Lineage
  Data and model lineage refer to the detailed record of the origin, transformation, and evolution of data and models used in AI and generative AI systems. This information is important for understanding the origin, reliability, and potential biases or limitations of the data and models used in these systems.
  • Source citation
      act of properly attributing and acknowledging the sources of the data used to train the model.
      It is necessary to identify the sources from which the training data was collected, such as the following: 
        • Datasets
        • Databases
        • Other sources
      In addition, it is necessary to identify any relevant licenses, terms of use, or permissions associated with the data.
      helps assess the "reliability" and "trustworthiness" of the output.
  • Documenting data origins
      providing detailed information about the provenance, or the place of origin of the data used to train the model.
      This includes the following:
        • Details about the data collection process
        • The methods used to curate and clean the data
        • Any preprocessing or transformations applied to the data
      Documenting the data origins is important for understanding the "potential biases, limitations, or quality issues" that might be present in the training data. This can ultimately impact the "performance" and "reliability" of the generative AI model.
  Tools and techniques
   • Data lineage
   • Cataloging
   • Model cards
   "Amazon SageMaker Model Cards"
      document critical details about your ML models in a single place for streamlined governance and reporting.
      Model cards can catalog details, such as the intended use and risk rating of a model, training details and metrics, evaluation results and observations.
      It also catalogs additional call-outs such as considerations, recommendations, and custom information.
      By creating model cards, you can do the following:
        • Provide guidance on how a model should be used.
        • Support audit activities with detailed descriptions of model training and performance.
        • Communicate how a model is intended to support business goals.

Best Practices for Secure Data Engineering
Review of data usage in generative AI
  • User data
      specific inputs or requirements provided by the customers or end-users. This data is used to generate or personalize the output of the generative AI model.
      For all application scopes(from Generative AI Security Scoping Matrix), the customer controls their data. 
  • Fine-tuning data
      This data is used to adapt or fine-tune the pre-trained a generative AI model to the specific needs or preferences of the customers or the application domain.
      The fine-tuning data is typically a subset of the training data or additional data collected from the application domain.
      The fine-tuning process adjusts the model's parameters and weights to better fit the fine-tuning data, allowing the model to generate more relevant and personalized outputs.
      For application Scopes 1 and 2, the application provider controls the fine-tuning data.
      For application Scope 4, the customer controls the fine-tuning data.
  • Training data
      comprehensive dataset used to train the initial pre-trained generative AI model.
      typically a large and diverse collection of data, such as text, images, or audio, depending on the specific application.
      used to build the fundamental knowledge and capabilities of the generative AI model.
      For application Scopes 1, 2, 3, and 4, the application provider controls the training data.
      For application Scope 5, the customer controls the training data.
Data flows in a generative AI application
  <DataFlowsGenAIapp-Final.png>
Data engineering lifecycle
  an iterative process where the data is collected, prepared, and analyzed. This data is then used to train, evaluate, and continuously improve the AI or generative AI models.
  <DataEngineeringLifeCycle-Final.png>
  • Data engineering automation and access control
      Pipeline automation is an important part of modern data-centric architecture design.
      You can use "AWS Glue workflows" to create a pipeline.
  • Data collection
      "Amazon Kinesis", "AWS Database Migration Service (DMS)" and AWS Glue
  • Data preparation and cleaning
      one of the most important, yet most time-consuming, stages of the data lifecycle.
      large workload that has a variety of data, use "Amazon EMR" or AWS Glue
  • Data quality checks
      "AWS Glue DataBrew", and "AWS Glue Data Quality"
  • Data visualization and analysis
      "Amazon QuickSight" - Use to create graphs or charts. 
      "Amazon Neptune" - Use for graph database operations and visualization.
  • Infrastructure as code (IaC) deployment
      "AWS CloudFormation"
  • Monitoring and debugging
      "Amazon CloudWatch"
5.1.c
Best Practices for Secure Data Engineering
  • Assessing data quality
    • Define clear data quality "metrics" and benchmarks such as Completeness, Accuracy, Timeliness and Consistency.
    • Implement "data validation checks" and tests at various stages of the data pipeline.
    • Perform regular data profiling and monitoring to identify data quality issues.
    • Establish a "feedback loop" to address data quality problems and continuously improve.
    • Maintain detailed "data lineage" and metadata to understand the origin and transformation of data.
  • Implementing privacy-enhancing technologies
    • Implement "data masking", data obfuscation, or differential privacy mechanisms to reduce the risk of data breaches.
    • Use "encryption", tokenization, or secure multi-party computation to protect data during processing and storage.
  • Data access control
    • Establish a comprehensive "data governance framework" with clear policies and procedures for data access, usage, and sharing.
    • Implement "role-based access" controls and fine-grained permissions to restrict access to sensitive data.
    • Use "authentication and authorization" mechanisms, such as single sign-on, MFA, or IAM solutions.
    • Monitor and "log all data access activities" to detect and investigate any unauthorized access or anomalies.
    • Regularly review and update access rights based on the principle of least privilege.
  • Data integrity
    quality, accuracy, and reliability of the data used to train the AI models.
    It ensures that the data used for model development, training, and deployment is complete, consistent, and free from errors or inconsistencies.
    • Implement data validation and "integrity checks" at various stages of the data pipeline, such as schema validation, referential integrity checks, and business rule validations.
    • Maintain a robust "data backup" and recovery strategy to ensure data can be restored in case of errors, system failures, or natural disasters.
    • Employ "transaction management" and "atomicity principles" to ensure data consistency and reliability during data processing and transformation.
    • Maintain detailed "data lineage" and audit trails to track the origin, transformations, and changes made to the data.
    • Regularly monitor and test the data integrity controls to ensure their effectiveness and make necessary adjustments.
  "AWS Privacy Reference Architecture"
    offers a set of guidelines to assist in the design and implementation of privacy-supporting controls within AWS services.
    helps make informed decisions regarding the people, processes, and technology that are necessary to ensure privacy in the AWS Cloud environment.
